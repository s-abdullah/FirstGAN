{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import numpy as np\n",
    "import imageio as img\n",
    "import math\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models\n",
    "from lib import vgan\n",
    "\n",
    "# helper functions\n",
    "from lib.helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unlike tensorflow where we have to define the graphs and their operations, in pytorch we deine the network as a class\n",
    "# i.e we just define the operations and the forward prop, the backprop is sorta in built\n",
    "\n",
    "# we get this done by defining our class that inherits fromthe torchnn.module and hence the other functions\n",
    "# such as backprop can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status of GPU:  True\n"
     ]
    }
   ],
   "source": [
    "print(\"Status of GPU: \", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "USE_CUDA = True \n",
    "MODEL_TYPE = \"DCGAN\"\n",
    "\n",
    "DATA_FOLDER = \"./alpha\"\n",
    "DATA_NAME = DATA_FOLDER.split('/')[-1].upper()\n",
    "\n",
    "# hyper parameters\n",
    "bSize = 100\n",
    "learning_rate = 0.0002\n",
    "\n",
    "dwidth = dheight = 64\n",
    "features = dwidth*dheight\n",
    "\n",
    "# DCGAN params\n",
    "channels = 1\n",
    "img_size = dwidth\n",
    "ndf = ngf = img_size\n",
    "latent_dim = 100\n",
    "\n",
    "# Number of steps to apply to the discriminator\n",
    "d_steps = 1  # In Goodfellow et. al 2014 this variable is assigned to 1\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom logger from (https://raw.githubusercontent.com/diegoalejogm/gans/master/utils.py) \n",
    "from lib.utils import Logger\n",
    "\n",
    "# Create logger instance\n",
    "logger = Logger(model_name=MODEL_TYPE, data_name=DATA_NAME, cuda=USE_CUDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is ``(nc) x 64 x 64``\n",
    "            nn.Conv2d(channels, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # state size. ``(ndf) x 32 x 32``\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # state size. ``(ndf*2) x 16 x 16``\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # state size. ``(ndf*4) x 8 x 8``\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # state size. ``(ndf*8) x 4 x 4``\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://arxiv.org/pdf/1511.06434, DCGAN\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.kernal_size = 4\n",
    "        self.stride = 2\n",
    "        self.padding = 1\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d( latent_dim, ngf * 8, \n",
    "                               kernel_size=self.kernal_size, \n",
    "                               stride=1, \n",
    "                               padding=0, \n",
    "                               bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # state size. ``(ngf*8) x 4 x 4``\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, \n",
    "                               kernel_size=self.kernal_size, \n",
    "                               stride=self.stride, \n",
    "                               padding=self.padding, \n",
    "                               bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # state size. ``(ngf*4) x 8 x 8``\n",
    "            nn.ConvTranspose2d( ngf * 4, ngf * 2, \n",
    "                               kernel_size=self.kernal_size, \n",
    "                               stride=self.stride, \n",
    "                               padding=self.padding, \n",
    "                               bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # state size. ``(ngf*2) x 16 x 16``\n",
    "            nn.ConvTranspose2d( ngf * 2, ngf, \n",
    "                               kernel_size=self.kernal_size, \n",
    "                               stride=self.stride, \n",
    "                               padding=self.padding, \n",
    "                               bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # state size. ``(ngf) x 32 x 32``\n",
    "            nn.ConvTranspose2d( ngf, channels, \n",
    "                               kernel_size=self.kernal_size, \n",
    "                               stride=self.stride, \n",
    "                               padding=self.padding, \n",
    "                               bias=False),\n",
    "            nn.Tanh()\n",
    "            \n",
    "            # state size. ``(nc) x 64 x 64``\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_cuda(thing):\n",
    "    if torch.cuda.is_available() and USE_CUDA: \n",
    "        return thing.cuda()\n",
    "    return thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator(optimizer, real_data, fake_data):\n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 1.1 Train on Real Data\n",
    "    prediction_real = discriminator(real_data)\n",
    "    # Calculate error and backpropagate\n",
    "    error_real = loss(prediction_real, to_cuda(real_data_target(real_data.size(0))))\n",
    "    error_real.backward()\n",
    "\n",
    "    # 1.2 Train on Fake Data\n",
    "    prediction_fake = discriminator(fake_data)\n",
    "    # Calculate error and backpropagate\n",
    "    error_fake = loss(prediction_fake, to_cuda(fake_data_target(real_data.size(0))))\n",
    "    error_fake.backward()\n",
    "    \n",
    "    # 1.3 Update weights with gradients\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Return error\n",
    "    return error_real + error_fake, prediction_real, prediction_fake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(optimizer, fake_data):\n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Sample noise and generate fake data\n",
    "    prediction = discriminator(fake_data)\n",
    "    \n",
    "    # Calculate error and backpropagate\n",
    "    error = loss(prediction, to_cuda(real_data_target(prediction.size(0))))\n",
    "    error.backward()\n",
    "    \n",
    "    # Update weights with gradients\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Return error\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_custom_data(data_folder):\n",
    "    compose = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Resize((dwidth, dheight)),\n",
    "         transforms.Grayscale(num_output_channels=1),\n",
    "         transforms.Normalize((.5), (.5))\n",
    "        ])\n",
    "\n",
    "    return datasets.ImageFolder(root=data_folder, transform=compose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data (https://github.com/soumith/ganhacks)\n",
    "def mnist_data():\n",
    "    compose = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Resize((dwidth, dheight)),\n",
    "         transforms.Normalize((.5), (.5))\n",
    "        ])\n",
    "    out_dir = '{}/dataset'.format(DATA_FOLDER)\n",
    "    return datasets.MNIST(root=out_dir, train=True, transform=compose, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "# data = mnist_data()\n",
    "\n",
    "# custom data\n",
    "folder = \"./alpha/AmplifiedDataset\"\n",
    "data = get_custom_data(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create loader with data, so that we can iterate over it\n",
    "data_loader = torch.utils.data.DataLoader(data, batch_size=bSize, shuffle=True)\n",
    "# Num batches\n",
    "num_batches = len(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Network Essentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla GAN\n",
    "# discriminator = to_cuda(vgan.Discriminator(features))\n",
    "# generator = to_cuda(vgan.Generator(bSize, features))\n",
    "\n",
    "# DCGAN\n",
    "discriminator = to_cuda(Discriminator())\n",
    "generator = to_cuda(Generator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# If we replace vᵢ = D(xᵢ) and yᵢ=1 ∀ i (for all i) in the BCE-Loss definition, \n",
    "# we obtain the loss related to the real-images. Conversely if we set vᵢ = D(G(zᵢ)) and yᵢ=0 ∀ i, \\\n",
    "# we obtain the loss related to the fake-images. \n",
    "# In the mathematical model of a GAN I described earlier, the gradient of this had to be ascended, \n",
    "# but PyTorch and most other Machine Learning frameworks usually minimize functions instead. \n",
    "# Since maximizing a function is equivalent to minimizing it’s negative, and the BCE-Loss term has a minus sign, \n",
    "# we don’t need to worry about the sign.\n",
    "# Loss function\n",
    "loss = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for testing Vanilla GAN\n",
    "num_test_samples = 16\n",
    "test_noise = noise(num_test_samples, bSize)\n",
    "\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     for n_batch, (real_batch,_) in enumerate(data_loader):\n",
    "#         N = real_batch.size(0)\n",
    "#         # 1. Train Discriminator\n",
    "#         real_data = to_cuda(Variable(images_to_vectors(real_batch, features)))\n",
    "        \n",
    "#         # Generate fake data and detach \n",
    "#         # (so gradients are not calculated for generator)\n",
    "#         # can use.clone() as well but it will be a copy\n",
    "#         # .detach() uses the same memory\n",
    "#         fake_data = generator(to_cuda(noise(N, bSize))).detach()\n",
    "        \n",
    "#         # Train D\n",
    "#         d_error, d_pred_real, d_pred_fake = \\\n",
    "#               train_discriminator(d_optimizer, real_data, fake_data)\n",
    "\n",
    "#         # 2. Train Generator\n",
    "#         # Generate fake data\n",
    "#         fake_data = generator(to_cuda(noise(N, bSize)))\n",
    "#         # Train G\n",
    "        \n",
    "#         g_error = train_generator(g_optimizer, fake_data)\n",
    "#         # Log batch error\n",
    "#         logger.log(d_error, g_error, epoch, n_batch, num_batches)\n",
    "        \n",
    "        \n",
    "#         # Display Progress every few batches\n",
    "#         if (n_batch) % 100 == 0: \n",
    "#             test_images = to_cuda(vectors_to_images(generator(test_noise), 1, dwidth, dheight))\n",
    "\n",
    "#             logger.log_images(\n",
    "#                 test_images, num_test_samples, \n",
    "#                 epoch, n_batch, num_batches\n",
    "#             );\n",
    "#             # Display status Logs\n",
    "#             logger.display_status(\n",
    "#                 epoch, num_epochs, n_batch, num_batches,\n",
    "#                 d_error, g_error, d_pred_real, d_pred_fake\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (12): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/200], Batch Num: [0/1335]\n",
      "Discriminator Loss: 0.7686, Generator Loss: 0.7478\n",
      "D(x): 0.8646, D(G(z)): 0.6726\n",
      "Epoch: [0/200], Batch Num: [100/1335]\n",
      "Discriminator Loss: 0.0007, Generator Loss: 6.9319\n",
      "D(x): 0.0005, D(G(z)): 0.0010\n",
      "Epoch: [0/200], Batch Num: [200/1335]\n",
      "Discriminator Loss: 0.0002, Generator Loss: 8.3806\n",
      "D(x): 0.0002, D(G(z)): 0.0002\n",
      "Epoch: [0/200], Batch Num: [300/1335]\n",
      "Discriminator Loss: 0.1181, Generator Loss: 2.1803\n",
      "D(x): 0.0005, D(G(z)): 0.2357\n",
      "Epoch: [0/200], Batch Num: [400/1335]\n",
      "Discriminator Loss: 0.0030, Generator Loss: 6.1239\n",
      "D(x): 0.0016, D(G(z)): 0.0044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20/200], Batch Num: [900/1335]\n",
      "Discriminator Loss: 0.0181, Generator Loss: 6.2439\n",
      "D(x): 0.0298, D(G(z)): 0.0065\n",
      "Epoch: [20/200], Batch Num: [1000/1335]\n",
      "Discriminator Loss: 0.0023, Generator Loss: 7.9438\n",
      "D(x): 0.0037, D(G(z)): 0.0010\n",
      "Epoch: [20/200], Batch Num: [1100/1335]\n",
      "Discriminator Loss: 0.0056, Generator Loss: 5.9021\n",
      "D(x): 0.0013, D(G(z)): 0.0099\n",
      "Epoch: [20/200], Batch Num: [1200/1335]\n",
      "Discriminator Loss: 0.0078, Generator Loss: 7.0037\n",
      "D(x): 0.0116, D(G(z)): 0.0040\n",
      "Epoch: [20/200], Batch Num: [1300/1335]\n",
      "Discriminator Loss: 0.0055, Generator Loss: 7.2955\n",
      "D(x): 0.0056, D(G(z)): 0.0054\n",
      "Epoch: [21/200], Batch Num: [65/1335]\n",
      "Discriminator Loss: 0.0043, Generator Loss: 7.2357\n",
      "D(x): 0.0062, D(G(z)): 0.0023\n",
      "Epoch: [21/200], Batch Num: [165/1335]\n",
      "Discriminator Loss: 0.0501, Generator Loss: 6.3967\n",
      "D(x): 0.0789, D(G(z)): 0.0214\n",
      "Epoch: [21/200], Batch Num: [265/1335]\n",
      "Discriminator Loss: 0.0478, Generator Loss: 4.3806\n",
      "D(x): 0.0166, D(G(z)): 0.0791\n",
      "Epoch: [21/200], Batch Num: [365/1335]\n",
      "Discriminator Loss: 0.0040, Generator Loss: 7.9229\n",
      "D(x): 0.0067, D(G(z)): 0.0014\n",
      "Epoch: [21/200], Batch Num: [465/1335]\n",
      "Discriminator Loss: 0.0023, Generator Loss: 7.2777\n",
      "D(x): 0.0025, D(G(z)): 0.0021\n",
      "Epoch: [21/200], Batch Num: [565/1335]\n",
      "Discriminator Loss: 0.0208, Generator Loss: 7.9107\n",
      "D(x): 0.0399, D(G(z)): 0.0018\n",
      "Epoch: [21/200], Batch Num: [665/1335]\n",
      "Discriminator Loss: 0.0143, Generator Loss: 5.3289\n",
      "D(x): 0.0088, D(G(z)): 0.0198\n",
      "Epoch: [21/200], Batch Num: [765/1335]\n",
      "Discriminator Loss: 0.0022, Generator Loss: 7.2518\n",
      "D(x): 0.0016, D(G(z)): 0.0028\n",
      "Epoch: [21/200], Batch Num: [865/1335]\n",
      "Discriminator Loss: 0.0158, Generator Loss: 4.7640\n",
      "D(x): 0.0005, D(G(z)): 0.0310\n",
      "Epoch: [21/200], Batch Num: [965/1335]\n",
      "Discriminator Loss: 0.0062, Generator Loss: 5.6473\n",
      "D(x): 0.0002, D(G(z)): 0.0122\n",
      "Epoch: [21/200], Batch Num: [1065/1335]\n",
      "Discriminator Loss: 0.0090, Generator Loss: 5.5254\n",
      "D(x): 0.0059, D(G(z)): 0.0120\n",
      "Epoch: [21/200], Batch Num: [1165/1335]\n",
      "Discriminator Loss: 0.0106, Generator Loss: 5.5342\n",
      "D(x): 0.0010, D(G(z)): 0.0201\n",
      "Epoch: [21/200], Batch Num: [1265/1335]\n",
      "Discriminator Loss: 0.0072, Generator Loss: 5.5062\n",
      "D(x): 0.0003, D(G(z)): 0.0141\n",
      "Epoch: [22/200], Batch Num: [30/1335]\n",
      "Discriminator Loss: 0.0125, Generator Loss: 9.8050\n",
      "D(x): 0.0247, D(G(z)): 0.0002\n",
      "Epoch: [22/200], Batch Num: [130/1335]\n",
      "Discriminator Loss: 0.0044, Generator Loss: 8.1188\n",
      "D(x): 0.0066, D(G(z)): 0.0021\n",
      "Epoch: [22/200], Batch Num: [230/1335]\n",
      "Discriminator Loss: 0.0066, Generator Loss: 6.3998\n",
      "D(x): 0.0012, D(G(z)): 0.0121\n",
      "Epoch: [22/200], Batch Num: [330/1335]\n",
      "Discriminator Loss: 0.0033, Generator Loss: 6.4606\n",
      "D(x): 0.0017, D(G(z)): 0.0049\n",
      "Epoch: [22/200], Batch Num: [430/1335]\n",
      "Discriminator Loss: 0.0350, Generator Loss: 6.3443\n",
      "D(x): 0.0311, D(G(z)): 0.0389\n",
      "Epoch: [22/200], Batch Num: [530/1335]\n",
      "Discriminator Loss: 0.2623, Generator Loss: 2.0373\n",
      "D(x): 0.0437, D(G(z)): 0.4810\n",
      "Epoch: [22/200], Batch Num: [630/1335]\n",
      "Discriminator Loss: 0.0326, Generator Loss: 5.8047\n",
      "D(x): 0.0532, D(G(z)): 0.0121\n",
      "Epoch: [22/200], Batch Num: [730/1335]\n",
      "Discriminator Loss: 0.0107, Generator Loss: 6.2977\n",
      "D(x): 0.0126, D(G(z)): 0.0089\n",
      "Epoch: [22/200], Batch Num: [830/1335]\n",
      "Discriminator Loss: 0.0054, Generator Loss: 6.0542\n",
      "D(x): 0.0038, D(G(z)): 0.0070\n",
      "Epoch: [22/200], Batch Num: [930/1335]\n",
      "Discriminator Loss: 0.0117, Generator Loss: 8.9497\n",
      "D(x): 0.0229, D(G(z)): 0.0006\n",
      "Epoch: [22/200], Batch Num: [1030/1335]\n",
      "Discriminator Loss: 0.0076, Generator Loss: 5.5439\n",
      "D(x): 0.0037, D(G(z)): 0.0114\n",
      "Epoch: [22/200], Batch Num: [1130/1335]\n",
      "Discriminator Loss: 0.0088, Generator Loss: 5.8092\n",
      "D(x): 0.0058, D(G(z)): 0.0117\n",
      "Epoch: [22/200], Batch Num: [1230/1335]\n",
      "Discriminator Loss: 0.0068, Generator Loss: 5.7311\n",
      "D(x): 0.0019, D(G(z)): 0.0117\n",
      "Epoch: [22/200], Batch Num: [1330/1335]\n",
      "Discriminator Loss: 0.0020, Generator Loss: 8.1616\n",
      "D(x): 0.0014, D(G(z)): 0.0026\n",
      "Epoch: [23/200], Batch Num: [95/1335]\n",
      "Discriminator Loss: 0.0018, Generator Loss: 7.6930\n",
      "D(x): 0.0024, D(G(z)): 0.0013\n",
      "Epoch: [23/200], Batch Num: [195/1335]\n",
      "Discriminator Loss: 0.0026, Generator Loss: 8.1705\n",
      "D(x): 0.0040, D(G(z)): 0.0012\n",
      "Epoch: [23/200], Batch Num: [295/1335]\n",
      "Discriminator Loss: 0.0042, Generator Loss: 7.3076\n",
      "D(x): 0.0042, D(G(z)): 0.0043\n",
      "Epoch: [23/200], Batch Num: [395/1335]\n",
      "Discriminator Loss: 0.0077, Generator Loss: 6.5529\n",
      "D(x): 0.0081, D(G(z)): 0.0073\n",
      "Epoch: [23/200], Batch Num: [495/1335]\n",
      "Discriminator Loss: 0.0113, Generator Loss: 8.2757\n",
      "D(x): 0.0217, D(G(z)): 0.0008\n",
      "Epoch: [23/200], Batch Num: [595/1335]\n",
      "Discriminator Loss: 0.0338, Generator Loss: 10.5073\n",
      "D(x): 0.0676, D(G(z)): 0.0001\n",
      "Epoch: [23/200], Batch Num: [695/1335]\n",
      "Discriminator Loss: 0.0083, Generator Loss: 7.2681\n",
      "D(x): 0.0132, D(G(z)): 0.0034\n",
      "Epoch: [23/200], Batch Num: [795/1335]\n",
      "Discriminator Loss: 0.0006, Generator Loss: 8.0778\n",
      "D(x): 0.0002, D(G(z)): 0.0010\n",
      "Epoch: [23/200], Batch Num: [895/1335]\n",
      "Discriminator Loss: 0.0447, Generator Loss: 9.0744\n",
      "D(x): 0.0882, D(G(z)): 0.0012\n",
      "Epoch: [23/200], Batch Num: [995/1335]\n",
      "Discriminator Loss: 0.2204, Generator Loss: 10.8232\n",
      "D(x): 0.4407, D(G(z)): 0.0002\n",
      "Epoch: [23/200], Batch Num: [1095/1335]\n",
      "Discriminator Loss: 0.0130, Generator Loss: 6.3834\n",
      "D(x): 0.0218, D(G(z)): 0.0041\n",
      "Epoch: [23/200], Batch Num: [1195/1335]\n",
      "Discriminator Loss: 0.0623, Generator Loss: 3.5620\n",
      "D(x): 0.0001, D(G(z)): 0.1246\n",
      "Epoch: [23/200], Batch Num: [1295/1335]\n",
      "Discriminator Loss: 0.0064, Generator Loss: 6.4291\n",
      "D(x): 0.0049, D(G(z)): 0.0079\n",
      "Epoch: [24/200], Batch Num: [60/1335]\n",
      "Discriminator Loss: 0.0033, Generator Loss: 9.0631\n",
      "D(x): 0.0063, D(G(z)): 0.0003\n",
      "Epoch: [24/200], Batch Num: [160/1335]\n",
      "Discriminator Loss: 0.0023, Generator Loss: 6.9633\n",
      "D(x): 0.0019, D(G(z)): 0.0027\n",
      "Epoch: [24/200], Batch Num: [260/1335]\n",
      "Discriminator Loss: 0.0039, Generator Loss: 6.5780\n",
      "D(x): 0.0026, D(G(z)): 0.0051\n",
      "Epoch: [24/200], Batch Num: [360/1335]\n",
      "Discriminator Loss: 0.0022, Generator Loss: 7.1754\n",
      "D(x): 0.0020, D(G(z)): 0.0024\n",
      "Epoch: [24/200], Batch Num: [460/1335]\n",
      "Discriminator Loss: 0.0034, Generator Loss: 6.6315\n",
      "D(x): 0.0023, D(G(z)): 0.0044\n",
      "Epoch: [24/200], Batch Num: [560/1335]\n",
      "Discriminator Loss: 0.0044, Generator Loss: 8.1657\n",
      "D(x): 0.0077, D(G(z)): 0.0011\n",
      "Epoch: [24/200], Batch Num: [660/1335]\n",
      "Discriminator Loss: 0.0032, Generator Loss: 7.1603\n",
      "D(x): 0.0040, D(G(z)): 0.0024\n",
      "Epoch: [24/200], Batch Num: [760/1335]\n",
      "Discriminator Loss: 0.0042, Generator Loss: 7.0671\n",
      "D(x): 0.0024, D(G(z)): 0.0060\n",
      "Epoch: [24/200], Batch Num: [860/1335]\n",
      "Discriminator Loss: 0.0029, Generator Loss: 7.0401\n",
      "D(x): 0.0025, D(G(z)): 0.0034\n",
      "Epoch: [24/200], Batch Num: [960/1335]\n",
      "Discriminator Loss: 0.0056, Generator Loss: 7.2875\n",
      "D(x): 0.0077, D(G(z)): 0.0035\n",
      "Epoch: [24/200], Batch Num: [1060/1335]\n",
      "Discriminator Loss: 0.0039, Generator Loss: 7.7975\n",
      "D(x): 0.0066, D(G(z)): 0.0012\n",
      "Epoch: [24/200], Batch Num: [1160/1335]\n",
      "Discriminator Loss: 0.0026, Generator Loss: 7.2512\n",
      "D(x): 0.0023, D(G(z)): 0.0029\n",
      "Epoch: [24/200], Batch Num: [1260/1335]\n",
      "Discriminator Loss: 0.0040, Generator Loss: 6.6404\n",
      "D(x): 0.0025, D(G(z)): 0.0055\n",
      "Epoch: [25/200], Batch Num: [25/1335]\n",
      "Discriminator Loss: 0.1681, Generator Loss: 3.9943\n",
      "D(x): 0.0002, D(G(z)): 0.3360\n",
      "Epoch: [25/200], Batch Num: [125/1335]\n",
      "Discriminator Loss: 0.1763, Generator Loss: 4.1588\n",
      "D(x): 0.2002, D(G(z)): 0.1523\n",
      "Epoch: [25/200], Batch Num: [225/1335]\n",
      "Discriminator Loss: 0.0276, Generator Loss: 6.1632\n",
      "D(x): 0.0451, D(G(z)): 0.0101\n",
      "Epoch: [25/200], Batch Num: [325/1335]\n",
      "Discriminator Loss: 0.0391, Generator Loss: 7.0457\n",
      "D(x): 0.0738, D(G(z)): 0.0045\n",
      "Epoch: [25/200], Batch Num: [425/1335]\n",
      "Discriminator Loss: 0.0050, Generator Loss: 6.0971\n",
      "D(x): 0.0035, D(G(z)): 0.0065\n",
      "Epoch: [25/200], Batch Num: [525/1335]\n",
      "Discriminator Loss: 0.0129, Generator Loss: 7.8080\n",
      "D(x): 0.0247, D(G(z)): 0.0011\n",
      "Epoch: [25/200], Batch Num: [625/1335]\n",
      "Discriminator Loss: 0.0085, Generator Loss: 6.3428\n",
      "D(x): 0.0109, D(G(z)): 0.0060\n",
      "Epoch: [25/200], Batch Num: [725/1335]\n",
      "Discriminator Loss: 0.0042, Generator Loss: 6.3954\n",
      "D(x): 0.0031, D(G(z)): 0.0052\n",
      "Epoch: [25/200], Batch Num: [825/1335]\n",
      "Discriminator Loss: 0.0026, Generator Loss: 7.3232\n",
      "D(x): 0.0032, D(G(z)): 0.0021\n",
      "Epoch: [25/200], Batch Num: [925/1335]\n",
      "Discriminator Loss: 0.0081, Generator Loss: 5.5926\n",
      "D(x): 0.0012, D(G(z)): 0.0150\n",
      "Epoch: [25/200], Batch Num: [1025/1335]\n",
      "Discriminator Loss: 0.0070, Generator Loss: 6.0033\n",
      "D(x): 0.0046, D(G(z)): 0.0093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [25/200], Batch Num: [1125/1335]\n",
      "Discriminator Loss: 0.0061, Generator Loss: 5.8599\n",
      "D(x): 0.0006, D(G(z)): 0.0117\n",
      "Epoch: [25/200], Batch Num: [1225/1335]\n",
      "Discriminator Loss: 0.0033, Generator Loss: 6.5058\n",
      "D(x): 0.0013, D(G(z)): 0.0054\n",
      "Epoch: [25/200], Batch Num: [1325/1335]\n",
      "Discriminator Loss: 0.0049, Generator Loss: 5.7825\n",
      "D(x): 0.0007, D(G(z)): 0.0092\n",
      "Epoch: [26/200], Batch Num: [90/1335]\n",
      "Discriminator Loss: 0.0030, Generator Loss: 8.1791\n",
      "D(x): 0.0053, D(G(z)): 0.0008\n",
      "Epoch: [26/200], Batch Num: [190/1335]\n",
      "Discriminator Loss: 0.0091, Generator Loss: 5.2512\n",
      "D(x): 0.0011, D(G(z)): 0.0170\n",
      "Epoch: [26/200], Batch Num: [290/1335]\n",
      "Discriminator Loss: 0.0026, Generator Loss: 8.3125\n",
      "D(x): 0.0031, D(G(z)): 0.0021\n",
      "Epoch: [26/200], Batch Num: [390/1335]\n",
      "Discriminator Loss: 0.0041, Generator Loss: 6.2691\n",
      "D(x): 0.0022, D(G(z)): 0.0059\n",
      "Epoch: [26/200], Batch Num: [490/1335]\n",
      "Discriminator Loss: 0.0047, Generator Loss: 6.7568\n",
      "D(x): 0.0024, D(G(z)): 0.0070\n",
      "Epoch: [26/200], Batch Num: [590/1335]\n",
      "Discriminator Loss: 0.0034, Generator Loss: 7.4053\n",
      "D(x): 0.0024, D(G(z)): 0.0044\n",
      "Epoch: [26/200], Batch Num: [690/1335]\n",
      "Discriminator Loss: 0.0174, Generator Loss: 6.5754\n",
      "D(x): 0.0175, D(G(z)): 0.0172\n",
      "Epoch: [26/200], Batch Num: [790/1335]\n",
      "Discriminator Loss: 0.0068, Generator Loss: 6.4647\n",
      "D(x): 0.0037, D(G(z)): 0.0100\n",
      "Epoch: [26/200], Batch Num: [890/1335]\n",
      "Discriminator Loss: 0.0498, Generator Loss: 4.4709\n",
      "D(x): 0.0166, D(G(z)): 0.0831\n",
      "Epoch: [26/200], Batch Num: [990/1335]\n",
      "Discriminator Loss: 0.0164, Generator Loss: 7.8068\n",
      "D(x): 0.0309, D(G(z)): 0.0019\n",
      "Epoch: [26/200], Batch Num: [1090/1335]\n",
      "Discriminator Loss: 0.0031, Generator Loss: 6.4237\n",
      "D(x): 0.0020, D(G(z)): 0.0043\n",
      "Epoch: [26/200], Batch Num: [1190/1335]\n",
      "Discriminator Loss: 0.0029, Generator Loss: 7.2060\n",
      "D(x): 0.0029, D(G(z)): 0.0029\n",
      "Epoch: [26/200], Batch Num: [1290/1335]\n",
      "Discriminator Loss: 0.0016, Generator Loss: 9.4984\n",
      "D(x): 0.0027, D(G(z)): 0.0004\n",
      "Epoch: [27/200], Batch Num: [55/1335]\n",
      "Discriminator Loss: 0.0021, Generator Loss: 7.6254\n",
      "D(x): 0.0018, D(G(z)): 0.0025\n",
      "Epoch: [27/200], Batch Num: [155/1335]\n",
      "Discriminator Loss: 0.0039, Generator Loss: 6.8282\n",
      "D(x): 0.0043, D(G(z)): 0.0035\n",
      "Epoch: [27/200], Batch Num: [255/1335]\n",
      "Discriminator Loss: 0.0018, Generator Loss: 6.9819\n",
      "D(x): 0.0008, D(G(z)): 0.0028\n",
      "Epoch: [27/200], Batch Num: [355/1335]\n",
      "Discriminator Loss: 0.0010, Generator Loss: 8.8978\n",
      "D(x): 0.0018, D(G(z)): 0.0003\n",
      "Epoch: [27/200], Batch Num: [455/1335]\n",
      "Discriminator Loss: 0.0017, Generator Loss: 7.7922\n",
      "D(x): 0.0019, D(G(z)): 0.0015\n",
      "Epoch: [27/200], Batch Num: [555/1335]\n",
      "Discriminator Loss: 0.0053, Generator Loss: 9.7644\n",
      "D(x): 0.0103, D(G(z)): 0.0003\n",
      "Epoch: [27/200], Batch Num: [655/1335]\n",
      "Discriminator Loss: 0.0074, Generator Loss: 7.1286\n",
      "D(x): 0.0125, D(G(z)): 0.0022\n",
      "Epoch: [27/200], Batch Num: [755/1335]\n",
      "Discriminator Loss: 0.0020, Generator Loss: 7.2739\n",
      "D(x): 0.0013, D(G(z)): 0.0027\n",
      "Epoch: [27/200], Batch Num: [855/1335]\n",
      "Discriminator Loss: 0.0012, Generator Loss: 8.0773\n",
      "D(x): 0.0012, D(G(z)): 0.0011\n",
      "Epoch: [27/200], Batch Num: [955/1335]\n",
      "Discriminator Loss: 0.0041, Generator Loss: 8.4757\n",
      "D(x): 0.0073, D(G(z)): 0.0008\n",
      "Epoch: [27/200], Batch Num: [1055/1335]\n",
      "Discriminator Loss: 0.0015, Generator Loss: 7.5660\n",
      "D(x): 0.0008, D(G(z)): 0.0022\n",
      "Epoch: [27/200], Batch Num: [1155/1335]\n",
      "Discriminator Loss: 0.0074, Generator Loss: 6.6752\n",
      "D(x): 0.0024, D(G(z)): 0.0125\n",
      "Epoch: [27/200], Batch Num: [1255/1335]\n",
      "Discriminator Loss: 0.2333, Generator Loss: 3.4399\n",
      "D(x): 0.0009, D(G(z)): 0.4656\n",
      "Epoch: [28/200], Batch Num: [20/1335]\n",
      "Discriminator Loss: 0.0121, Generator Loss: 6.0398\n",
      "D(x): 0.0101, D(G(z)): 0.0141\n",
      "Epoch: [28/200], Batch Num: [120/1335]\n",
      "Discriminator Loss: 0.0066, Generator Loss: 6.2273\n",
      "D(x): 0.0059, D(G(z)): 0.0074\n",
      "Epoch: [28/200], Batch Num: [220/1335]\n",
      "Discriminator Loss: 0.0040, Generator Loss: 6.1918\n",
      "D(x): 0.0024, D(G(z)): 0.0056\n",
      "Epoch: [28/200], Batch Num: [320/1335]\n",
      "Discriminator Loss: 0.0146, Generator Loss: 6.2039\n",
      "D(x): 0.0136, D(G(z)): 0.0156\n",
      "Epoch: [28/200], Batch Num: [420/1335]\n",
      "Discriminator Loss: 0.0082, Generator Loss: 5.7398\n",
      "D(x): 0.0061, D(G(z)): 0.0104\n",
      "Epoch: [28/200], Batch Num: [520/1335]\n",
      "Discriminator Loss: 0.0085, Generator Loss: 5.8142\n",
      "D(x): 0.0062, D(G(z)): 0.0108\n",
      "Epoch: [28/200], Batch Num: [620/1335]\n",
      "Discriminator Loss: 0.0117, Generator Loss: 6.0058\n",
      "D(x): 0.0082, D(G(z)): 0.0153\n",
      "Epoch: [28/200], Batch Num: [720/1335]\n",
      "Discriminator Loss: 0.0024, Generator Loss: 8.0402\n",
      "D(x): 0.0040, D(G(z)): 0.0009\n",
      "Epoch: [28/200], Batch Num: [820/1335]\n",
      "Discriminator Loss: 0.0083, Generator Loss: 5.5039\n",
      "D(x): 0.0002, D(G(z)): 0.0164\n",
      "Epoch: [28/200], Batch Num: [920/1335]\n",
      "Discriminator Loss: 0.0028, Generator Loss: 8.2989\n",
      "D(x): 0.0047, D(G(z)): 0.0008\n",
      "Epoch: [28/200], Batch Num: [1020/1335]\n",
      "Discriminator Loss: 0.0085, Generator Loss: 5.2526\n",
      "D(x): 0.0006, D(G(z)): 0.0164\n",
      "Epoch: [28/200], Batch Num: [1120/1335]\n",
      "Discriminator Loss: 0.0047, Generator Loss: 7.3674\n",
      "D(x): 0.0069, D(G(z)): 0.0025\n",
      "Epoch: [28/200], Batch Num: [1220/1335]\n",
      "Discriminator Loss: 0.0022, Generator Loss: 6.6321\n",
      "D(x): 0.0003, D(G(z)): 0.0041\n",
      "Epoch: [28/200], Batch Num: [1320/1335]\n",
      "Discriminator Loss: 0.0019, Generator Loss: 7.3417\n",
      "D(x): 0.0018, D(G(z)): 0.0021\n",
      "Epoch: [29/200], Batch Num: [85/1335]\n",
      "Discriminator Loss: 0.0032, Generator Loss: 6.3171\n",
      "D(x): 0.0008, D(G(z)): 0.0056\n",
      "Epoch: [29/200], Batch Num: [185/1335]\n",
      "Discriminator Loss: 0.0011, Generator Loss: 8.1813\n",
      "D(x): 0.0013, D(G(z)): 0.0010\n",
      "Epoch: [29/200], Batch Num: [285/1335]\n",
      "Discriminator Loss: 0.0024, Generator Loss: 8.9460\n",
      "D(x): 0.0044, D(G(z)): 0.0005\n",
      "Epoch: [29/200], Batch Num: [385/1335]\n",
      "Discriminator Loss: 0.0012, Generator Loss: 7.6318\n",
      "D(x): 0.0008, D(G(z)): 0.0015\n",
      "Epoch: [29/200], Batch Num: [485/1335]\n",
      "Discriminator Loss: 0.0039, Generator Loss: 9.0485\n",
      "D(x): 0.0073, D(G(z)): 0.0004\n",
      "Epoch: [29/200], Batch Num: [585/1335]\n",
      "Discriminator Loss: 0.0035, Generator Loss: 6.2677\n",
      "D(x): 0.0003, D(G(z)): 0.0067\n",
      "Epoch: [29/200], Batch Num: [685/1335]\n",
      "Discriminator Loss: 0.0017, Generator Loss: 7.8585\n",
      "D(x): 0.0021, D(G(z)): 0.0014\n",
      "Epoch: [29/200], Batch Num: [785/1335]\n",
      "Discriminator Loss: 0.0010, Generator Loss: 7.8163\n",
      "D(x): 0.0010, D(G(z)): 0.0011\n",
      "Epoch: [29/200], Batch Num: [885/1335]\n",
      "Discriminator Loss: 0.0047, Generator Loss: 6.4562\n",
      "D(x): 0.0007, D(G(z)): 0.0086\n",
      "Epoch: [29/200], Batch Num: [985/1335]\n",
      "Discriminator Loss: 0.3738, Generator Loss: 2.7633\n",
      "D(x): 0.2592, D(G(z)): 0.4883\n",
      "Epoch: [29/200], Batch Num: [1085/1335]\n",
      "Discriminator Loss: 0.0151, Generator Loss: 5.5304\n",
      "D(x): 0.0119, D(G(z)): 0.0183\n",
      "Epoch: [29/200], Batch Num: [1185/1335]\n",
      "Discriminator Loss: 0.0395, Generator Loss: 5.8809\n",
      "D(x): 0.0687, D(G(z)): 0.0103\n",
      "Epoch: [29/200], Batch Num: [1285/1335]\n",
      "Discriminator Loss: 0.0056, Generator Loss: 5.9988\n",
      "D(x): 0.0024, D(G(z)): 0.0087\n",
      "Epoch: [30/200], Batch Num: [50/1335]\n",
      "Discriminator Loss: 0.0683, Generator Loss: 3.1905\n",
      "D(x): 0.0011, D(G(z)): 0.1354\n",
      "Epoch: [30/200], Batch Num: [150/1335]\n",
      "Discriminator Loss: 0.0181, Generator Loss: 5.5599\n",
      "D(x): 0.0196, D(G(z)): 0.0167\n",
      "Epoch: [30/200], Batch Num: [250/1335]\n",
      "Discriminator Loss: 0.0055, Generator Loss: 8.0442\n",
      "D(x): 0.0082, D(G(z)): 0.0028\n",
      "Epoch: [30/200], Batch Num: [350/1335]\n",
      "Discriminator Loss: 0.0062, Generator Loss: 6.8827\n",
      "D(x): 0.0076, D(G(z)): 0.0048\n",
      "Epoch: [30/200], Batch Num: [450/1335]\n",
      "Discriminator Loss: 0.0077, Generator Loss: 6.4846\n",
      "D(x): 0.0071, D(G(z)): 0.0082\n",
      "Epoch: [30/200], Batch Num: [550/1335]\n",
      "Discriminator Loss: 0.0067, Generator Loss: 8.7148\n",
      "D(x): 0.0127, D(G(z)): 0.0008\n",
      "Epoch: [30/200], Batch Num: [650/1335]\n",
      "Discriminator Loss: 0.0052, Generator Loss: 7.3928\n",
      "D(x): 0.0086, D(G(z)): 0.0018\n",
      "Epoch: [30/200], Batch Num: [750/1335]\n",
      "Discriminator Loss: 0.0040, Generator Loss: 6.9922\n",
      "D(x): 0.0049, D(G(z)): 0.0030\n",
      "Epoch: [30/200], Batch Num: [850/1335]\n",
      "Discriminator Loss: 0.0055, Generator Loss: 6.4168\n",
      "D(x): 0.0023, D(G(z)): 0.0088\n",
      "Epoch: [30/200], Batch Num: [950/1335]\n",
      "Discriminator Loss: 0.0027, Generator Loss: 7.5867\n",
      "D(x): 0.0034, D(G(z)): 0.0021\n",
      "Epoch: [30/200], Batch Num: [1050/1335]\n",
      "Discriminator Loss: 0.0032, Generator Loss: 7.0185\n",
      "D(x): 0.0035, D(G(z)): 0.0028\n",
      "Epoch: [30/200], Batch Num: [1150/1335]\n",
      "Discriminator Loss: 0.0238, Generator Loss: 5.3047\n",
      "D(x): 0.0025, D(G(z)): 0.0451\n",
      "Epoch: [30/200], Batch Num: [1250/1335]\n",
      "Discriminator Loss: 0.0101, Generator Loss: 6.8969\n",
      "D(x): 0.0144, D(G(z)): 0.0059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [31/200], Batch Num: [15/1335]\n",
      "Discriminator Loss: 0.0074, Generator Loss: 6.9722\n",
      "D(x): 0.0017, D(G(z)): 0.0131\n",
      "Epoch: [31/200], Batch Num: [115/1335]\n",
      "Discriminator Loss: 0.0056, Generator Loss: 5.9451\n",
      "D(x): 0.0016, D(G(z)): 0.0095\n",
      "Epoch: [31/200], Batch Num: [215/1335]\n",
      "Discriminator Loss: 0.0033, Generator Loss: 6.6603\n",
      "D(x): 0.0011, D(G(z)): 0.0055\n",
      "Epoch: [31/200], Batch Num: [315/1335]\n",
      "Discriminator Loss: 0.0032, Generator Loss: 6.6289\n",
      "D(x): 0.0005, D(G(z)): 0.0060\n",
      "Epoch: [31/200], Batch Num: [415/1335]\n",
      "Discriminator Loss: 0.0013, Generator Loss: 7.9720\n",
      "D(x): 0.0016, D(G(z)): 0.0010\n",
      "Epoch: [31/200], Batch Num: [515/1335]\n",
      "Discriminator Loss: 0.0015, Generator Loss: 7.3926\n",
      "D(x): 0.0006, D(G(z)): 0.0023\n",
      "Epoch: [31/200], Batch Num: [615/1335]\n",
      "Discriminator Loss: 0.0267, Generator Loss: 8.3294\n",
      "D(x): 0.0408, D(G(z)): 0.0125\n",
      "Epoch: [31/200], Batch Num: [715/1335]\n",
      "Discriminator Loss: 0.0372, Generator Loss: 8.2548\n",
      "D(x): 0.0722, D(G(z)): 0.0022\n",
      "Epoch: [31/200], Batch Num: [815/1335]\n",
      "Discriminator Loss: 0.0297, Generator Loss: 6.5511\n",
      "D(x): 0.0481, D(G(z)): 0.0114\n",
      "Epoch: [31/200], Batch Num: [915/1335]\n",
      "Discriminator Loss: 0.0750, Generator Loss: 4.6349\n",
      "D(x): 0.0655, D(G(z)): 0.0844\n",
      "Epoch: [31/200], Batch Num: [1015/1335]\n",
      "Discriminator Loss: 0.0081, Generator Loss: 6.6052\n",
      "D(x): 0.0062, D(G(z)): 0.0099\n",
      "Epoch: [31/200], Batch Num: [1115/1335]\n",
      "Discriminator Loss: 0.0177, Generator Loss: 9.2552\n",
      "D(x): 0.0348, D(G(z)): 0.0007\n",
      "Epoch: [31/200], Batch Num: [1215/1335]\n",
      "Discriminator Loss: 0.0039, Generator Loss: 8.5051\n",
      "D(x): 0.0072, D(G(z)): 0.0007\n",
      "Epoch: [31/200], Batch Num: [1315/1335]\n",
      "Discriminator Loss: 0.0026, Generator Loss: 7.6592\n",
      "D(x): 0.0041, D(G(z)): 0.0012\n",
      "Epoch: [32/200], Batch Num: [80/1335]\n",
      "Discriminator Loss: 0.0007, Generator Loss: 8.7014\n",
      "D(x): 0.0008, D(G(z)): 0.0006\n",
      "Epoch: [32/200], Batch Num: [180/1335]\n",
      "Discriminator Loss: 0.0039, Generator Loss: 6.9133\n",
      "D(x): 0.0046, D(G(z)): 0.0033\n",
      "Epoch: [32/200], Batch Num: [280/1335]\n",
      "Discriminator Loss: 0.0092, Generator Loss: 7.9607\n",
      "D(x): 0.0170, D(G(z)): 0.0014\n",
      "Epoch: [32/200], Batch Num: [380/1335]\n",
      "Discriminator Loss: 0.0081, Generator Loss: 5.7413\n",
      "D(x): 0.0036, D(G(z)): 0.0127\n",
      "Epoch: [32/200], Batch Num: [480/1335]\n",
      "Discriminator Loss: 0.0027, Generator Loss: 6.9541\n",
      "D(x): 0.0005, D(G(z)): 0.0049\n",
      "Epoch: [32/200], Batch Num: [580/1335]\n",
      "Discriminator Loss: 0.0030, Generator Loss: 6.3567\n",
      "D(x): 0.0013, D(G(z)): 0.0047\n",
      "Epoch: [32/200], Batch Num: [680/1335]\n",
      "Discriminator Loss: 0.0049, Generator Loss: 5.8822\n",
      "D(x): 0.0008, D(G(z)): 0.0090\n",
      "Epoch: [32/200], Batch Num: [780/1335]\n",
      "Discriminator Loss: 0.0004, Generator Loss: 9.5515\n",
      "D(x): 0.0005, D(G(z)): 0.0003\n",
      "Epoch: [32/200], Batch Num: [880/1335]\n",
      "Discriminator Loss: 0.0026, Generator Loss: 7.9862\n",
      "D(x): 0.0041, D(G(z)): 0.0011\n",
      "Epoch: [32/200], Batch Num: [980/1335]\n",
      "Discriminator Loss: 0.0035, Generator Loss: 6.6052\n",
      "D(x): 0.0019, D(G(z)): 0.0051\n",
      "Epoch: [32/200], Batch Num: [1080/1335]\n",
      "Discriminator Loss: 0.0015, Generator Loss: 11.2401\n",
      "D(x): 0.0028, D(G(z)): 0.0001\n",
      "Epoch: [32/200], Batch Num: [1180/1335]\n",
      "Discriminator Loss: 0.0436, Generator Loss: 8.3312\n",
      "D(x): 0.0791, D(G(z)): 0.0081\n",
      "Epoch: [32/200], Batch Num: [1280/1335]\n",
      "Discriminator Loss: 0.0505, Generator Loss: 4.6944\n",
      "D(x): 0.0030, D(G(z)): 0.0979\n",
      "Epoch: [33/200], Batch Num: [45/1335]\n",
      "Discriminator Loss: 0.0025, Generator Loss: 7.0595\n",
      "D(x): 0.0013, D(G(z)): 0.0037\n",
      "Epoch: [33/200], Batch Num: [145/1335]\n",
      "Discriminator Loss: 0.0035, Generator Loss: 7.1078\n",
      "D(x): 0.0031, D(G(z)): 0.0039\n",
      "Epoch: [33/200], Batch Num: [245/1335]\n",
      "Discriminator Loss: 0.0059, Generator Loss: 6.5086\n",
      "D(x): 0.0067, D(G(z)): 0.0051\n",
      "Epoch: [33/200], Batch Num: [345/1335]\n",
      "Discriminator Loss: 0.0022, Generator Loss: 8.5228\n",
      "D(x): 0.0033, D(G(z)): 0.0010\n",
      "Epoch: [33/200], Batch Num: [445/1335]\n",
      "Discriminator Loss: 0.0013, Generator Loss: 8.8813\n",
      "D(x): 0.0021, D(G(z)): 0.0006\n",
      "Epoch: [33/200], Batch Num: [545/1335]\n",
      "Discriminator Loss: 0.0012, Generator Loss: 7.7195\n",
      "D(x): 0.0009, D(G(z)): 0.0014\n",
      "Epoch: [33/200], Batch Num: [645/1335]\n",
      "Discriminator Loss: 0.0021, Generator Loss: 7.9933\n",
      "D(x): 0.0033, D(G(z)): 0.0010\n",
      "Epoch: [33/200], Batch Num: [745/1335]\n",
      "Discriminator Loss: 0.0036, Generator Loss: 6.5084\n",
      "D(x): 0.0025, D(G(z)): 0.0047\n",
      "Epoch: [33/200], Batch Num: [845/1335]\n",
      "Discriminator Loss: 0.0006, Generator Loss: 8.7522\n",
      "D(x): 0.0006, D(G(z)): 0.0006\n",
      "Epoch: [33/200], Batch Num: [945/1335]\n",
      "Discriminator Loss: 0.0098, Generator Loss: 10.1389\n",
      "D(x): 0.0193, D(G(z)): 0.0002\n",
      "Epoch: [33/200], Batch Num: [1045/1335]\n",
      "Discriminator Loss: 0.0072, Generator Loss: 6.9134\n",
      "D(x): 0.0010, D(G(z)): 0.0133\n",
      "Epoch: [33/200], Batch Num: [1145/1335]\n",
      "Discriminator Loss: 0.0049, Generator Loss: 6.5697\n",
      "D(x): 0.0020, D(G(z)): 0.0079\n",
      "Epoch: [33/200], Batch Num: [1245/1335]\n",
      "Discriminator Loss: 0.0011, Generator Loss: 8.2623\n",
      "D(x): 0.0013, D(G(z)): 0.0009\n",
      "Epoch: [34/200], Batch Num: [10/1335]\n",
      "Discriminator Loss: 0.0012, Generator Loss: 7.6081\n",
      "D(x): 0.0000, D(G(z)): 0.0023\n",
      "Epoch: [34/200], Batch Num: [110/1335]\n",
      "Discriminator Loss: 0.0033, Generator Loss: 8.1475\n",
      "D(x): 0.0056, D(G(z)): 0.0009\n",
      "Epoch: [34/200], Batch Num: [210/1335]\n",
      "Discriminator Loss: 0.0120, Generator Loss: 9.1475\n",
      "D(x): 0.0235, D(G(z)): 0.0005\n",
      "Epoch: [34/200], Batch Num: [310/1335]\n",
      "Discriminator Loss: 0.0408, Generator Loss: 6.5754\n",
      "D(x): 0.0065, D(G(z)): 0.0750\n",
      "Epoch: [34/200], Batch Num: [410/1335]\n",
      "Discriminator Loss: 0.0309, Generator Loss: 7.5291\n",
      "D(x): 0.0475, D(G(z)): 0.0143\n",
      "Epoch: [34/200], Batch Num: [510/1335]\n",
      "Discriminator Loss: 0.0169, Generator Loss: 7.1234\n",
      "D(x): 0.0307, D(G(z)): 0.0032\n",
      "Epoch: [34/200], Batch Num: [610/1335]\n",
      "Discriminator Loss: 0.0088, Generator Loss: 7.0409\n",
      "D(x): 0.0058, D(G(z)): 0.0118\n",
      "Epoch: [34/200], Batch Num: [710/1335]\n",
      "Discriminator Loss: 0.0029, Generator Loss: 7.8771\n",
      "D(x): 0.0045, D(G(z)): 0.0013\n",
      "Epoch: [34/200], Batch Num: [810/1335]\n",
      "Discriminator Loss: 0.0015, Generator Loss: 7.5498\n",
      "D(x): 0.0012, D(G(z)): 0.0019\n",
      "Epoch: [34/200], Batch Num: [910/1335]\n",
      "Discriminator Loss: 0.0075, Generator Loss: 6.0125\n",
      "D(x): 0.0008, D(G(z)): 0.0142\n",
      "Epoch: [34/200], Batch Num: [1010/1335]\n",
      "Discriminator Loss: 0.0031, Generator Loss: 7.1038\n",
      "D(x): 0.0019, D(G(z)): 0.0043\n",
      "Epoch: [34/200], Batch Num: [1110/1335]\n",
      "Discriminator Loss: 0.0050, Generator Loss: 5.7927\n",
      "D(x): 0.0019, D(G(z)): 0.0081\n",
      "Epoch: [34/200], Batch Num: [1210/1335]\n",
      "Discriminator Loss: 0.0035, Generator Loss: 7.0768\n",
      "D(x): 0.0035, D(G(z)): 0.0034\n",
      "Epoch: [34/200], Batch Num: [1310/1335]\n",
      "Discriminator Loss: 0.0012, Generator Loss: 8.4166\n",
      "D(x): 0.0014, D(G(z)): 0.0010\n",
      "Epoch: [35/200], Batch Num: [75/1335]\n",
      "Discriminator Loss: 0.0050, Generator Loss: 6.0411\n",
      "D(x): 0.0012, D(G(z)): 0.0087\n",
      "Epoch: [35/200], Batch Num: [175/1335]\n",
      "Discriminator Loss: 0.0008, Generator Loss: 9.0136\n",
      "D(x): 0.0011, D(G(z)): 0.0004\n",
      "Epoch: [35/200], Batch Num: [275/1335]\n",
      "Discriminator Loss: 0.0014, Generator Loss: 8.2661\n",
      "D(x): 0.0020, D(G(z)): 0.0009\n",
      "Epoch: [35/200], Batch Num: [375/1335]\n",
      "Discriminator Loss: 0.0011, Generator Loss: 7.7631\n",
      "D(x): 0.0006, D(G(z)): 0.0016\n",
      "Epoch: [35/200], Batch Num: [475/1335]\n",
      "Discriminator Loss: 0.0017, Generator Loss: 8.2411\n",
      "D(x): 0.0006, D(G(z)): 0.0029\n",
      "Epoch: [35/200], Batch Num: [575/1335]\n",
      "Discriminator Loss: 0.0058, Generator Loss: 9.8616\n",
      "D(x): 0.0112, D(G(z)): 0.0003\n",
      "Epoch: [35/200], Batch Num: [675/1335]\n",
      "Discriminator Loss: 0.0042, Generator Loss: 10.3068\n",
      "D(x): 0.0082, D(G(z)): 0.0003\n",
      "Epoch: [35/200], Batch Num: [775/1335]\n",
      "Discriminator Loss: 0.0056, Generator Loss: 7.4925\n",
      "D(x): 0.0014, D(G(z)): 0.0099\n",
      "Epoch: [35/200], Batch Num: [875/1335]\n",
      "Discriminator Loss: 0.0015, Generator Loss: 9.9906\n",
      "D(x): 0.0025, D(G(z)): 0.0005\n",
      "Epoch: [35/200], Batch Num: [975/1335]\n",
      "Discriminator Loss: 0.0411, Generator Loss: 9.3549\n",
      "D(x): 0.0784, D(G(z)): 0.0037\n",
      "Epoch: [35/200], Batch Num: [1075/1335]\n",
      "Discriminator Loss: 0.1494, Generator Loss: 9.8477\n",
      "D(x): 0.2983, D(G(z)): 0.0005\n",
      "Epoch: [35/200], Batch Num: [1175/1335]\n",
      "Discriminator Loss: 0.0192, Generator Loss: 5.4684\n",
      "D(x): 0.0005, D(G(z)): 0.0378\n",
      "Epoch: [35/200], Batch Num: [1275/1335]\n",
      "Discriminator Loss: 0.0272, Generator Loss: 4.8019\n",
      "D(x): 0.0018, D(G(z)): 0.0526\n",
      "Epoch: [36/200], Batch Num: [40/1335]\n",
      "Discriminator Loss: 0.0061, Generator Loss: 7.3943\n",
      "D(x): 0.0043, D(G(z)): 0.0079\n",
      "Epoch: [36/200], Batch Num: [140/1335]\n",
      "Discriminator Loss: 0.0030, Generator Loss: 7.6925\n",
      "D(x): 0.0046, D(G(z)): 0.0014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [36/200], Batch Num: [240/1335]\n",
      "Discriminator Loss: 0.0026, Generator Loss: 6.8084\n",
      "D(x): 0.0003, D(G(z)): 0.0049\n",
      "Epoch: [36/200], Batch Num: [340/1335]\n",
      "Discriminator Loss: 0.0059, Generator Loss: 6.9041\n",
      "D(x): 0.0074, D(G(z)): 0.0044\n",
      "Epoch: [36/200], Batch Num: [440/1335]\n",
      "Discriminator Loss: 0.0086, Generator Loss: 7.9311\n",
      "D(x): 0.0157, D(G(z)): 0.0015\n",
      "Epoch: [36/200], Batch Num: [540/1335]\n",
      "Discriminator Loss: 0.0137, Generator Loss: 5.2424\n",
      "D(x): 0.0003, D(G(z)): 0.0271\n",
      "Epoch: [36/200], Batch Num: [640/1335]\n",
      "Discriminator Loss: 0.0109, Generator Loss: 7.3207\n",
      "D(x): 0.0186, D(G(z)): 0.0032\n",
      "Epoch: [36/200], Batch Num: [740/1335]\n",
      "Discriminator Loss: 0.0012, Generator Loss: 10.4048\n",
      "D(x): 0.0023, D(G(z)): 0.0002\n",
      "Epoch: [36/200], Batch Num: [840/1335]\n",
      "Discriminator Loss: 0.0023, Generator Loss: 7.7573\n",
      "D(x): 0.0028, D(G(z)): 0.0017\n",
      "Epoch: [36/200], Batch Num: [940/1335]\n",
      "Discriminator Loss: 0.0019, Generator Loss: 7.5121\n",
      "D(x): 0.0013, D(G(z)): 0.0025\n",
      "Epoch: [36/200], Batch Num: [1040/1335]\n",
      "Discriminator Loss: 0.0025, Generator Loss: 6.6274\n",
      "D(x): 0.0001, D(G(z)): 0.0049\n",
      "Epoch: [36/200], Batch Num: [1140/1335]\n",
      "Discriminator Loss: 0.0044, Generator Loss: 7.1153\n",
      "D(x): 0.0042, D(G(z)): 0.0046\n",
      "Epoch: [36/200], Batch Num: [1240/1335]\n",
      "Discriminator Loss: 0.0039, Generator Loss: 6.3661\n",
      "D(x): 0.0024, D(G(z)): 0.0053\n",
      "Epoch: [37/200], Batch Num: [5/1335]\n",
      "Discriminator Loss: 0.0115, Generator Loss: 8.5853\n",
      "D(x): 0.0205, D(G(z)): 0.0025\n",
      "Epoch: [37/200], Batch Num: [105/1335]\n",
      "Discriminator Loss: 0.0018, Generator Loss: 8.0263\n",
      "D(x): 0.0015, D(G(z)): 0.0022\n",
      "Epoch: [37/200], Batch Num: [205/1335]\n",
      "Discriminator Loss: 0.0061, Generator Loss: 5.9857\n",
      "D(x): 0.0003, D(G(z)): 0.0119\n",
      "Epoch: [37/200], Batch Num: [305/1335]\n",
      "Discriminator Loss: 0.0133, Generator Loss: 5.1240\n",
      "D(x): 0.0009, D(G(z)): 0.0256\n",
      "Epoch: [37/200], Batch Num: [405/1335]\n",
      "Discriminator Loss: 0.0004, Generator Loss: 9.1863\n",
      "D(x): 0.0003, D(G(z)): 0.0006\n",
      "Epoch: [37/200], Batch Num: [505/1335]\n",
      "Discriminator Loss: 0.0078, Generator Loss: 7.7016\n",
      "D(x): 0.0141, D(G(z)): 0.0015\n",
      "Epoch: [37/200], Batch Num: [605/1335]\n",
      "Discriminator Loss: 0.0012, Generator Loss: 7.8972\n",
      "D(x): 0.0002, D(G(z)): 0.0022\n",
      "Epoch: [37/200], Batch Num: [705/1335]\n",
      "Discriminator Loss: 0.0013, Generator Loss: 7.9609\n",
      "D(x): 0.0002, D(G(z)): 0.0024\n",
      "Epoch: [37/200], Batch Num: [805/1335]\n",
      "Discriminator Loss: 0.0006, Generator Loss: 8.2414\n",
      "D(x): 0.0000, D(G(z)): 0.0011\n",
      "Epoch: [37/200], Batch Num: [905/1335]\n",
      "Discriminator Loss: 0.0019, Generator Loss: 8.0704\n",
      "D(x): 0.0026, D(G(z)): 0.0012\n",
      "Epoch: [37/200], Batch Num: [1005/1335]\n",
      "Discriminator Loss: 0.0010, Generator Loss: 7.8086\n",
      "D(x): 0.0002, D(G(z)): 0.0017\n",
      "Epoch: [37/200], Batch Num: [1105/1335]\n",
      "Discriminator Loss: 0.0139, Generator Loss: 6.7027\n",
      "D(x): 0.0099, D(G(z)): 0.0179\n",
      "Epoch: [37/200], Batch Num: [1205/1335]\n",
      "Discriminator Loss: 0.0169, Generator Loss: 7.2543\n",
      "D(x): 0.0039, D(G(z)): 0.0299\n",
      "Epoch: [37/200], Batch Num: [1305/1335]\n",
      "Discriminator Loss: 0.0388, Generator Loss: 4.1642\n",
      "D(x): 0.0009, D(G(z)): 0.0768\n",
      "Epoch: [38/200], Batch Num: [70/1335]\n",
      "Discriminator Loss: 0.0416, Generator Loss: 8.2527\n",
      "D(x): 0.0818, D(G(z)): 0.0014\n",
      "Epoch: [38/200], Batch Num: [170/1335]\n",
      "Discriminator Loss: 0.0072, Generator Loss: 5.7976\n",
      "D(x): 0.0018, D(G(z)): 0.0125\n",
      "Epoch: [38/200], Batch Num: [270/1335]\n",
      "Discriminator Loss: 0.0085, Generator Loss: 8.1633\n",
      "D(x): 0.0162, D(G(z)): 0.0009\n",
      "Epoch: [38/200], Batch Num: [370/1335]\n",
      "Discriminator Loss: 0.0009, Generator Loss: 8.7907\n",
      "D(x): 0.0011, D(G(z)): 0.0006\n",
      "Epoch: [38/200], Batch Num: [470/1335]\n",
      "Discriminator Loss: 0.0893, Generator Loss: 8.2226\n",
      "D(x): 0.1778, D(G(z)): 0.0008\n",
      "Epoch: [38/200], Batch Num: [570/1335]\n",
      "Discriminator Loss: 0.0020, Generator Loss: 7.4284\n",
      "D(x): 0.0015, D(G(z)): 0.0025\n",
      "Epoch: [38/200], Batch Num: [670/1335]\n",
      "Discriminator Loss: 0.0019, Generator Loss: 8.2769\n",
      "D(x): 0.0025, D(G(z)): 0.0013\n",
      "Epoch: [38/200], Batch Num: [770/1335]\n",
      "Discriminator Loss: 0.0045, Generator Loss: 6.9072\n",
      "D(x): 0.0060, D(G(z)): 0.0029\n",
      "Epoch: [38/200], Batch Num: [870/1335]\n",
      "Discriminator Loss: 0.0036, Generator Loss: 6.2823\n",
      "D(x): 0.0002, D(G(z)): 0.0069\n",
      "Epoch: [38/200], Batch Num: [970/1335]\n",
      "Discriminator Loss: 0.0007, Generator Loss: 8.3079\n",
      "D(x): 0.0006, D(G(z)): 0.0009\n",
      "Epoch: [38/200], Batch Num: [1070/1335]\n",
      "Discriminator Loss: 0.0012, Generator Loss: 10.2437\n",
      "D(x): 0.0024, D(G(z)): 0.0001\n",
      "Epoch: [38/200], Batch Num: [1170/1335]\n",
      "Discriminator Loss: 0.0030, Generator Loss: 8.0401\n",
      "D(x): 0.0049, D(G(z)): 0.0012\n",
      "Epoch: [38/200], Batch Num: [1270/1335]\n",
      "Discriminator Loss: 0.0020, Generator Loss: 7.2024\n",
      "D(x): 0.0008, D(G(z)): 0.0032\n",
      "Epoch: [39/200], Batch Num: [35/1335]\n",
      "Discriminator Loss: 0.0037, Generator Loss: 7.5664\n",
      "D(x): 0.0004, D(G(z)): 0.0070\n",
      "Epoch: [39/200], Batch Num: [135/1335]\n",
      "Discriminator Loss: 0.0019, Generator Loss: 7.4856\n",
      "D(x): 0.0017, D(G(z)): 0.0020\n",
      "Epoch: [39/200], Batch Num: [235/1335]\n",
      "Discriminator Loss: 0.0015, Generator Loss: 8.6568\n",
      "D(x): 0.0023, D(G(z)): 0.0007\n",
      "Epoch: [39/200], Batch Num: [335/1335]\n",
      "Discriminator Loss: 0.0003, Generator Loss: 10.5301\n",
      "D(x): 0.0005, D(G(z)): 0.0001\n",
      "Epoch: [39/200], Batch Num: [435/1335]\n",
      "Discriminator Loss: 0.0003, Generator Loss: 9.6486\n",
      "D(x): 0.0003, D(G(z)): 0.0003\n",
      "Epoch: [39/200], Batch Num: [535/1335]\n",
      "Discriminator Loss: 0.0110, Generator Loss: 10.0981\n",
      "D(x): 0.0217, D(G(z)): 0.0003\n",
      "Epoch: [39/200], Batch Num: [635/1335]\n",
      "Discriminator Loss: 0.0037, Generator Loss: 8.7669\n",
      "D(x): 0.0067, D(G(z)): 0.0006\n",
      "Epoch: [39/200], Batch Num: [735/1335]\n",
      "Discriminator Loss: 0.0008, Generator Loss: 7.7964\n",
      "D(x): 0.0001, D(G(z)): 0.0015\n",
      "Epoch: [39/200], Batch Num: [835/1335]\n",
      "Discriminator Loss: 0.0026, Generator Loss: 7.9824\n",
      "D(x): 0.0030, D(G(z)): 0.0022\n",
      "Epoch: [39/200], Batch Num: [935/1335]\n",
      "Discriminator Loss: 0.0037, Generator Loss: 6.6177\n",
      "D(x): 0.0009, D(G(z)): 0.0066\n",
      "Epoch: [39/200], Batch Num: [1035/1335]\n",
      "Discriminator Loss: 0.1343, Generator Loss: 4.2978\n",
      "D(x): 0.0015, D(G(z)): 0.2671\n",
      "Epoch: [39/200], Batch Num: [1135/1335]\n",
      "Discriminator Loss: 0.0244, Generator Loss: 7.1014\n",
      "D(x): 0.0330, D(G(z)): 0.0158\n",
      "Epoch: [39/200], Batch Num: [1235/1335]\n",
      "Discriminator Loss: 0.0078, Generator Loss: 5.7372\n",
      "D(x): 0.0043, D(G(z)): 0.0113\n",
      "Epoch: [40/200], Batch Num: [0/1335]\n",
      "Discriminator Loss: 0.0017, Generator Loss: 7.0705\n",
      "D(x): 0.0006, D(G(z)): 0.0027\n",
      "Epoch: [40/200], Batch Num: [100/1335]\n",
      "Discriminator Loss: 0.0036, Generator Loss: 7.0078\n",
      "D(x): 0.0030, D(G(z)): 0.0042\n",
      "Epoch: [40/200], Batch Num: [200/1335]\n",
      "Discriminator Loss: 0.0044, Generator Loss: 5.9705\n",
      "D(x): 0.0024, D(G(z)): 0.0065\n",
      "Epoch: [40/200], Batch Num: [300/1335]\n",
      "Discriminator Loss: 0.0031, Generator Loss: 11.0208\n",
      "D(x): 0.0061, D(G(z)): 0.0001\n",
      "Epoch: [40/200], Batch Num: [400/1335]\n",
      "Discriminator Loss: 0.0016, Generator Loss: 8.0432\n",
      "D(x): 0.0004, D(G(z)): 0.0028\n",
      "Epoch: [40/200], Batch Num: [500/1335]\n",
      "Discriminator Loss: 0.0039, Generator Loss: 7.6604\n",
      "D(x): 0.0057, D(G(z)): 0.0021\n",
      "Epoch: [40/200], Batch Num: [600/1335]\n",
      "Discriminator Loss: 0.0016, Generator Loss: 8.3673\n",
      "D(x): 0.0016, D(G(z)): 0.0015\n",
      "Epoch: [40/200], Batch Num: [700/1335]\n",
      "Discriminator Loss: 0.0051, Generator Loss: 6.3269\n",
      "D(x): 0.0003, D(G(z)): 0.0099\n",
      "Epoch: [40/200], Batch Num: [800/1335]\n",
      "Discriminator Loss: 0.0047, Generator Loss: 6.0587\n",
      "D(x): 0.0019, D(G(z)): 0.0075\n",
      "Epoch: [40/200], Batch Num: [900/1335]\n",
      "Discriminator Loss: 0.0269, Generator Loss: 6.2147\n",
      "D(x): 0.0420, D(G(z)): 0.0117\n",
      "Epoch: [40/200], Batch Num: [1000/1335]\n",
      "Discriminator Loss: 0.0019, Generator Loss: 7.0220\n",
      "D(x): 0.0005, D(G(z)): 0.0032\n",
      "Epoch: [40/200], Batch Num: [1100/1335]\n",
      "Discriminator Loss: 0.0008, Generator Loss: 8.6014\n",
      "D(x): 0.0007, D(G(z)): 0.0009\n",
      "Epoch: [40/200], Batch Num: [1200/1335]\n",
      "Discriminator Loss: 0.0015, Generator Loss: 7.3192\n",
      "D(x): 0.0003, D(G(z)): 0.0028\n",
      "Epoch: [40/200], Batch Num: [1300/1335]\n",
      "Discriminator Loss: 0.0018, Generator Loss: 7.1080\n",
      "D(x): 0.0011, D(G(z)): 0.0024\n",
      "Epoch: [41/200], Batch Num: [65/1335]\n",
      "Discriminator Loss: 0.0018, Generator Loss: 7.2878\n",
      "D(x): 0.0002, D(G(z)): 0.0034\n",
      "Epoch: [41/200], Batch Num: [165/1335]\n",
      "Discriminator Loss: 0.0004, Generator Loss: 10.5162\n",
      "D(x): 0.0005, D(G(z)): 0.0003\n",
      "Epoch: [41/200], Batch Num: [265/1335]\n",
      "Discriminator Loss: 0.0011, Generator Loss: 8.5347\n",
      "D(x): 0.0001, D(G(z)): 0.0022\n",
      "Epoch: [41/200], Batch Num: [365/1335]\n",
      "Discriminator Loss: 0.0005, Generator Loss: 10.6223\n",
      "D(x): 0.0010, D(G(z)): 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [41/200], Batch Num: [465/1335]\n",
      "Discriminator Loss: 0.0552, Generator Loss: 6.8181\n",
      "D(x): 0.0281, D(G(z)): 0.0823\n",
      "Epoch: [41/200], Batch Num: [565/1335]\n",
      "Discriminator Loss: 0.0186, Generator Loss: 7.4286\n",
      "D(x): 0.0341, D(G(z)): 0.0031\n",
      "Epoch: [41/200], Batch Num: [665/1335]\n",
      "Discriminator Loss: 0.0264, Generator Loss: 6.9242\n",
      "D(x): 0.0431, D(G(z)): 0.0096\n",
      "Epoch: [41/200], Batch Num: [765/1335]\n",
      "Discriminator Loss: 0.0091, Generator Loss: 5.4279\n",
      "D(x): 0.0024, D(G(z)): 0.0157\n",
      "Epoch: [41/200], Batch Num: [865/1335]\n",
      "Discriminator Loss: 0.0032, Generator Loss: 7.6900\n",
      "D(x): 0.0046, D(G(z)): 0.0018\n",
      "Epoch: [41/200], Batch Num: [965/1335]\n",
      "Discriminator Loss: 0.0052, Generator Loss: 8.2775\n",
      "D(x): 0.0090, D(G(z)): 0.0013\n",
      "Epoch: [41/200], Batch Num: [1065/1335]\n",
      "Discriminator Loss: 0.0008, Generator Loss: 9.2164\n",
      "D(x): 0.0011, D(G(z)): 0.0006\n",
      "Epoch: [41/200], Batch Num: [1165/1335]\n",
      "Discriminator Loss: 0.0015, Generator Loss: 6.9511\n",
      "D(x): 0.0001, D(G(z)): 0.0029\n",
      "Epoch: [41/200], Batch Num: [1265/1335]\n",
      "Discriminator Loss: 0.0036, Generator Loss: 7.5511\n",
      "D(x): 0.0040, D(G(z)): 0.0032\n",
      "Epoch: [42/200], Batch Num: [30/1335]\n",
      "Discriminator Loss: 0.0029, Generator Loss: 9.7785\n",
      "D(x): 0.0056, D(G(z)): 0.0002\n",
      "Epoch: [42/200], Batch Num: [130/1335]\n",
      "Discriminator Loss: 0.0010, Generator Loss: 8.6438\n",
      "D(x): 0.0014, D(G(z)): 0.0006\n",
      "Epoch: [42/200], Batch Num: [230/1335]\n",
      "Discriminator Loss: 0.0003, Generator Loss: 9.3940\n",
      "D(x): 0.0003, D(G(z)): 0.0003\n",
      "Epoch: [42/200], Batch Num: [330/1335]\n",
      "Discriminator Loss: 0.0038, Generator Loss: 7.0819\n",
      "D(x): 0.0040, D(G(z)): 0.0036\n",
      "Epoch: [42/200], Batch Num: [430/1335]\n",
      "Discriminator Loss: 0.0038, Generator Loss: 9.5477\n",
      "D(x): 0.0070, D(G(z)): 0.0007\n",
      "Epoch: [42/200], Batch Num: [530/1335]\n",
      "Discriminator Loss: 0.0076, Generator Loss: 6.6567\n",
      "D(x): 0.0006, D(G(z)): 0.0146\n",
      "Epoch: [42/200], Batch Num: [630/1335]\n",
      "Discriminator Loss: 0.0018, Generator Loss: 8.5149\n",
      "D(x): 0.0028, D(G(z)): 0.0007\n",
      "Epoch: [42/200], Batch Num: [730/1335]\n",
      "Discriminator Loss: 0.0047, Generator Loss: 7.9956\n",
      "D(x): 0.0079, D(G(z)): 0.0014\n",
      "Epoch: [42/200], Batch Num: [830/1335]\n",
      "Discriminator Loss: 0.0019, Generator Loss: 7.1164\n",
      "D(x): 0.0009, D(G(z)): 0.0028\n",
      "Epoch: [42/200], Batch Num: [930/1335]\n",
      "Discriminator Loss: 0.0003, Generator Loss: 9.7057\n",
      "D(x): 0.0005, D(G(z)): 0.0002\n",
      "Epoch: [42/200], Batch Num: [1030/1335]\n",
      "Discriminator Loss: 0.0007, Generator Loss: 8.2021\n",
      "D(x): 0.0003, D(G(z)): 0.0010\n",
      "Epoch: [42/200], Batch Num: [1130/1335]\n",
      "Discriminator Loss: 0.0030, Generator Loss: 9.6370\n",
      "D(x): 0.0057, D(G(z)): 0.0003\n",
      "Epoch: [42/200], Batch Num: [1230/1335]\n",
      "Discriminator Loss: 0.0007, Generator Loss: 9.1010\n",
      "D(x): 0.0010, D(G(z)): 0.0005\n",
      "Epoch: [42/200], Batch Num: [1330/1335]\n",
      "Discriminator Loss: 0.0006, Generator Loss: 9.0934\n",
      "D(x): 0.0006, D(G(z)): 0.0006\n",
      "Epoch: [43/200], Batch Num: [95/1335]\n",
      "Discriminator Loss: 0.0019, Generator Loss: 8.3855\n",
      "D(x): 0.0028, D(G(z)): 0.0009\n",
      "Epoch: [43/200], Batch Num: [195/1335]\n",
      "Discriminator Loss: 0.0037, Generator Loss: 9.1489\n",
      "D(x): 0.0070, D(G(z)): 0.0004\n",
      "Epoch: [43/200], Batch Num: [295/1335]\n",
      "Discriminator Loss: 0.0238, Generator Loss: 7.0743\n",
      "D(x): 0.0337, D(G(z)): 0.0140\n",
      "Epoch: [43/200], Batch Num: [395/1335]\n",
      "Discriminator Loss: 0.3015, Generator Loss: 10.0372\n",
      "D(x): 0.5915, D(G(z)): 0.0116\n",
      "Epoch: [43/200], Batch Num: [495/1335]\n",
      "Discriminator Loss: 0.0520, Generator Loss: 10.2038\n",
      "D(x): 0.1039, D(G(z)): 0.0002\n",
      "Epoch: [43/200], Batch Num: [595/1335]\n",
      "Discriminator Loss: 0.0203, Generator Loss: 5.6888\n",
      "D(x): 0.0256, D(G(z)): 0.0149\n",
      "Epoch: [43/200], Batch Num: [695/1335]\n",
      "Discriminator Loss: 0.0060, Generator Loss: 6.7587\n",
      "D(x): 0.0080, D(G(z)): 0.0040\n",
      "Epoch: [43/200], Batch Num: [795/1335]\n",
      "Discriminator Loss: 0.0035, Generator Loss: 6.3746\n",
      "D(x): 0.0024, D(G(z)): 0.0047\n",
      "Epoch: [43/200], Batch Num: [895/1335]\n",
      "Discriminator Loss: 0.0057, Generator Loss: 6.9286\n",
      "D(x): 0.0083, D(G(z)): 0.0030\n",
      "Epoch: [43/200], Batch Num: [995/1335]\n",
      "Discriminator Loss: 0.0370, Generator Loss: 9.8878\n",
      "D(x): 0.0731, D(G(z)): 0.0009\n",
      "Epoch: [43/200], Batch Num: [1095/1335]\n",
      "Discriminator Loss: 0.0022, Generator Loss: 7.2874\n",
      "D(x): 0.0021, D(G(z)): 0.0023\n",
      "Epoch: [43/200], Batch Num: [1195/1335]\n",
      "Discriminator Loss: 0.0025, Generator Loss: 7.7286\n",
      "D(x): 0.0034, D(G(z)): 0.0017\n",
      "Epoch: [43/200], Batch Num: [1295/1335]\n",
      "Discriminator Loss: 0.0037, Generator Loss: 6.2306\n",
      "D(x): 0.0010, D(G(z)): 0.0064\n",
      "Epoch: [44/200], Batch Num: [60/1335]\n",
      "Discriminator Loss: 0.0051, Generator Loss: 6.2457\n",
      "D(x): 0.0023, D(G(z)): 0.0078\n",
      "Epoch: [44/200], Batch Num: [160/1335]\n",
      "Discriminator Loss: 0.0093, Generator Loss: 5.4469\n",
      "D(x): 0.0012, D(G(z)): 0.0174\n",
      "Epoch: [44/200], Batch Num: [260/1335]\n",
      "Discriminator Loss: 0.0017, Generator Loss: 6.7633\n",
      "D(x): 0.0002, D(G(z)): 0.0032\n",
      "Epoch: [44/200], Batch Num: [360/1335]\n",
      "Discriminator Loss: 0.0014, Generator Loss: 7.7502\n",
      "D(x): 0.0017, D(G(z)): 0.0012\n",
      "Epoch: [44/200], Batch Num: [460/1335]\n",
      "Discriminator Loss: 0.0042, Generator Loss: 9.1523\n",
      "D(x): 0.0080, D(G(z)): 0.0003\n",
      "Epoch: [44/200], Batch Num: [560/1335]\n",
      "Discriminator Loss: 0.0082, Generator Loss: 5.3510\n",
      "D(x): 0.0002, D(G(z)): 0.0162\n",
      "Epoch: [44/200], Batch Num: [660/1335]\n",
      "Discriminator Loss: 0.0048, Generator Loss: 7.2500\n",
      "D(x): 0.0044, D(G(z)): 0.0052\n",
      "Epoch: [44/200], Batch Num: [760/1335]\n",
      "Discriminator Loss: 0.0016, Generator Loss: 7.8064\n",
      "D(x): 0.0013, D(G(z)): 0.0018\n",
      "Epoch: [44/200], Batch Num: [860/1335]\n",
      "Discriminator Loss: 0.0022, Generator Loss: 10.7482\n",
      "D(x): 0.0043, D(G(z)): 0.0001\n",
      "Epoch: [44/200], Batch Num: [960/1335]\n",
      "Discriminator Loss: 0.0016, Generator Loss: 8.7452\n",
      "D(x): 0.0009, D(G(z)): 0.0022\n",
      "Epoch: [44/200], Batch Num: [1060/1335]\n",
      "Discriminator Loss: 0.0011, Generator Loss: 8.7992\n",
      "D(x): 0.0016, D(G(z)): 0.0006\n",
      "Epoch: [44/200], Batch Num: [1160/1335]\n",
      "Discriminator Loss: 0.0027, Generator Loss: 6.9882\n",
      "D(x): 0.0009, D(G(z)): 0.0045\n",
      "Epoch: [44/200], Batch Num: [1260/1335]\n",
      "Discriminator Loss: 0.0085, Generator Loss: 6.0385\n",
      "D(x): 0.0002, D(G(z)): 0.0168\n",
      "Epoch: [45/200], Batch Num: [25/1335]\n",
      "Discriminator Loss: 0.0008, Generator Loss: 9.0513\n",
      "D(x): 0.0012, D(G(z)): 0.0004\n",
      "Epoch: [45/200], Batch Num: [125/1335]\n",
      "Discriminator Loss: 0.0008, Generator Loss: 8.4959\n",
      "D(x): 0.0007, D(G(z)): 0.0009\n",
      "Epoch: [45/200], Batch Num: [225/1335]\n",
      "Discriminator Loss: 0.0410, Generator Loss: 6.9244\n",
      "D(x): 0.0744, D(G(z)): 0.0077\n",
      "Epoch: [45/200], Batch Num: [325/1335]\n",
      "Discriminator Loss: 0.0241, Generator Loss: 6.5065\n",
      "D(x): 0.0381, D(G(z)): 0.0100\n",
      "Epoch: [45/200], Batch Num: [425/1335]\n",
      "Discriminator Loss: 0.0208, Generator Loss: 4.9552\n",
      "D(x): 0.0021, D(G(z)): 0.0396\n",
      "Epoch: [45/200], Batch Num: [525/1335]\n",
      "Discriminator Loss: 0.0237, Generator Loss: 8.6647\n",
      "D(x): 0.0461, D(G(z)): 0.0013\n",
      "Epoch: [45/200], Batch Num: [625/1335]\n",
      "Discriminator Loss: 0.0146, Generator Loss: 9.2954\n",
      "D(x): 0.0288, D(G(z)): 0.0004\n",
      "Epoch: [45/200], Batch Num: [725/1335]\n",
      "Discriminator Loss: 0.0017, Generator Loss: 7.5875\n",
      "D(x): 0.0003, D(G(z)): 0.0031\n",
      "Epoch: [45/200], Batch Num: [825/1335]\n",
      "Discriminator Loss: 0.0011, Generator Loss: 9.3158\n",
      "D(x): 0.0019, D(G(z)): 0.0003\n",
      "Epoch: [45/200], Batch Num: [925/1335]\n",
      "Discriminator Loss: 0.0016, Generator Loss: 7.7007\n",
      "D(x): 0.0012, D(G(z)): 0.0020\n",
      "Epoch: [45/200], Batch Num: [1025/1335]\n",
      "Discriminator Loss: 0.0009, Generator Loss: 9.4683\n",
      "D(x): 0.0013, D(G(z)): 0.0005\n",
      "Epoch: [45/200], Batch Num: [1125/1335]\n",
      "Discriminator Loss: 0.0004, Generator Loss: 10.2884\n",
      "D(x): 0.0006, D(G(z)): 0.0003\n",
      "Epoch: [45/200], Batch Num: [1225/1335]\n",
      "Discriminator Loss: 0.0026, Generator Loss: 8.7321\n",
      "D(x): 0.0048, D(G(z)): 0.0004\n",
      "Epoch: [45/200], Batch Num: [1325/1335]\n",
      "Discriminator Loss: 0.0041, Generator Loss: 6.5954\n",
      "D(x): 0.0007, D(G(z)): 0.0074\n",
      "Epoch: [46/200], Batch Num: [90/1335]\n",
      "Discriminator Loss: 0.0017, Generator Loss: 8.4478\n",
      "D(x): 0.0028, D(G(z)): 0.0006\n",
      "Epoch: [46/200], Batch Num: [190/1335]\n",
      "Discriminator Loss: 0.0085, Generator Loss: 9.2493\n",
      "D(x): 0.0166, D(G(z)): 0.0004\n",
      "Epoch: [46/200], Batch Num: [290/1335]\n",
      "Discriminator Loss: 0.0012, Generator Loss: 7.9142\n",
      "D(x): 0.0012, D(G(z)): 0.0013\n",
      "Epoch: [46/200], Batch Num: [390/1335]\n",
      "Discriminator Loss: 0.0028, Generator Loss: 7.1209\n",
      "D(x): 0.0012, D(G(z)): 0.0044\n",
      "Epoch: [46/200], Batch Num: [490/1335]\n",
      "Discriminator Loss: 0.0006, Generator Loss: 10.1559\n",
      "D(x): 0.0010, D(G(z)): 0.0002\n",
      "Epoch: [46/200], Batch Num: [590/1335]\n",
      "Discriminator Loss: 0.0007, Generator Loss: 9.0213\n",
      "D(x): 0.0010, D(G(z)): 0.0004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [46/200], Batch Num: [690/1335]\n",
      "Discriminator Loss: 0.0006, Generator Loss: 8.8085\n",
      "D(x): 0.0007, D(G(z)): 0.0005\n",
      "Epoch: [46/200], Batch Num: [790/1335]\n",
      "Discriminator Loss: 0.0385, Generator Loss: 4.7089\n",
      "D(x): 0.0005, D(G(z)): 0.0765\n",
      "Epoch: [46/200], Batch Num: [890/1335]\n",
      "Discriminator Loss: 1.6970, Generator Loss: 0.5058\n",
      "D(x): 0.0002, D(G(z)): 3.3939\n",
      "Epoch: [46/200], Batch Num: [990/1335]\n",
      "Discriminator Loss: 0.0058, Generator Loss: 6.6047\n",
      "D(x): 0.0031, D(G(z)): 0.0084\n",
      "Epoch: [46/200], Batch Num: [1090/1335]\n",
      "Discriminator Loss: 0.0105, Generator Loss: 7.8534\n",
      "D(x): 0.0194, D(G(z)): 0.0017\n",
      "Epoch: [46/200], Batch Num: [1190/1335]\n",
      "Discriminator Loss: 0.0101, Generator Loss: 7.7790\n",
      "D(x): 0.0185, D(G(z)): 0.0018\n",
      "Epoch: [46/200], Batch Num: [1290/1335]\n",
      "Discriminator Loss: 0.0106, Generator Loss: 6.6090\n",
      "D(x): 0.0159, D(G(z)): 0.0053\n",
      "Epoch: [47/200], Batch Num: [55/1335]\n",
      "Discriminator Loss: 0.0024, Generator Loss: 6.7175\n",
      "D(x): 0.0002, D(G(z)): 0.0046\n",
      "Epoch: [47/200], Batch Num: [155/1335]\n",
      "Discriminator Loss: 0.0023, Generator Loss: 6.9590\n",
      "D(x): 0.0011, D(G(z)): 0.0035\n",
      "Epoch: [47/200], Batch Num: [255/1335]\n",
      "Discriminator Loss: 0.0045, Generator Loss: 6.8371\n",
      "D(x): 0.0023, D(G(z)): 0.0066\n",
      "Epoch: [47/200], Batch Num: [355/1335]\n",
      "Discriminator Loss: 0.0055, Generator Loss: 8.4276\n",
      "D(x): 0.0099, D(G(z)): 0.0011\n",
      "Epoch: [47/200], Batch Num: [455/1335]\n",
      "Discriminator Loss: 0.0021, Generator Loss: 8.5800\n",
      "D(x): 0.0031, D(G(z)): 0.0010\n",
      "Epoch: [47/200], Batch Num: [555/1335]\n",
      "Discriminator Loss: 0.0020, Generator Loss: 8.2621\n",
      "D(x): 0.0030, D(G(z)): 0.0010\n",
      "Epoch: [47/200], Batch Num: [655/1335]\n",
      "Discriminator Loss: 0.0049, Generator Loss: 6.5800\n",
      "D(x): 0.0034, D(G(z)): 0.0064\n",
      "Epoch: [47/200], Batch Num: [755/1335]\n",
      "Discriminator Loss: 0.0013, Generator Loss: 11.0159\n",
      "D(x): 0.0025, D(G(z)): 0.0001\n",
      "Epoch: [47/200], Batch Num: [855/1335]\n",
      "Discriminator Loss: 0.0042, Generator Loss: 6.5131\n",
      "D(x): 0.0033, D(G(z)): 0.0051\n",
      "Epoch: [47/200], Batch Num: [955/1335]\n",
      "Discriminator Loss: 0.0004, Generator Loss: 9.4022\n",
      "D(x): 0.0006, D(G(z)): 0.0003\n",
      "Epoch: [47/200], Batch Num: [1055/1335]\n",
      "Discriminator Loss: 0.0057, Generator Loss: 5.6499\n",
      "D(x): 0.0012, D(G(z)): 0.0103\n",
      "Epoch: [47/200], Batch Num: [1155/1335]\n",
      "Discriminator Loss: 0.0032, Generator Loss: 7.1784\n",
      "D(x): 0.0030, D(G(z)): 0.0034\n",
      "Epoch: [47/200], Batch Num: [1255/1335]\n",
      "Discriminator Loss: 0.0014, Generator Loss: 7.6020\n",
      "D(x): 0.0006, D(G(z)): 0.0022\n",
      "Epoch: [48/200], Batch Num: [20/1335]\n",
      "Discriminator Loss: 0.0037, Generator Loss: 6.4068\n",
      "D(x): 0.0015, D(G(z)): 0.0060\n",
      "Epoch: [48/200], Batch Num: [120/1335]\n",
      "Discriminator Loss: 0.0291, Generator Loss: 4.0464\n",
      "D(x): 0.0001, D(G(z)): 0.0582\n",
      "Epoch: [48/200], Batch Num: [220/1335]\n",
      "Discriminator Loss: 0.0011, Generator Loss: 7.6524\n",
      "D(x): 0.0002, D(G(z)): 0.0020\n",
      "Epoch: [48/200], Batch Num: [320/1335]\n",
      "Discriminator Loss: 0.0011, Generator Loss: 8.8470\n",
      "D(x): 0.0014, D(G(z)): 0.0008\n",
      "Epoch: [48/200], Batch Num: [420/1335]\n",
      "Discriminator Loss: 0.0022, Generator Loss: 7.5173\n",
      "D(x): 0.0006, D(G(z)): 0.0038\n",
      "Epoch: [48/200], Batch Num: [520/1335]\n",
      "Discriminator Loss: 0.0019, Generator Loss: 6.7662\n",
      "D(x): 0.0006, D(G(z)): 0.0033\n",
      "Epoch: [48/200], Batch Num: [620/1335]\n",
      "Discriminator Loss: 0.0011, Generator Loss: 9.3132\n",
      "D(x): 0.0017, D(G(z)): 0.0005\n",
      "Epoch: [48/200], Batch Num: [720/1335]\n",
      "Discriminator Loss: 0.0012, Generator Loss: 8.8302\n",
      "D(x): 0.0017, D(G(z)): 0.0007\n",
      "Epoch: [48/200], Batch Num: [820/1335]\n",
      "Discriminator Loss: 0.0010, Generator Loss: 7.9907\n",
      "D(x): 0.0004, D(G(z)): 0.0016\n",
      "Epoch: [48/200], Batch Num: [920/1335]\n",
      "Discriminator Loss: 0.0004, Generator Loss: 10.3750\n",
      "D(x): 0.0007, D(G(z)): 0.0002\n",
      "Epoch: [48/200], Batch Num: [1020/1335]\n",
      "Discriminator Loss: 0.0340, Generator Loss: 10.7246\n",
      "D(x): 0.0679, D(G(z)): 0.0001\n",
      "Epoch: [48/200], Batch Num: [1120/1335]\n",
      "Discriminator Loss: 0.0906, Generator Loss: 3.7115\n",
      "D(x): 0.0212, D(G(z)): 0.1600\n",
      "Epoch: [48/200], Batch Num: [1220/1335]\n",
      "Discriminator Loss: 0.0120, Generator Loss: 5.7237\n",
      "D(x): 0.0047, D(G(z)): 0.0193\n",
      "Epoch: [48/200], Batch Num: [1320/1335]\n",
      "Discriminator Loss: 0.0905, Generator Loss: 2.8560\n",
      "D(x): 0.0001, D(G(z)): 0.1810\n",
      "Epoch: [49/200], Batch Num: [85/1335]\n",
      "Discriminator Loss: 0.0115, Generator Loss: 9.1981\n",
      "D(x): 0.0226, D(G(z)): 0.0003\n",
      "Epoch: [49/200], Batch Num: [185/1335]\n",
      "Discriminator Loss: 0.0222, Generator Loss: 4.4628\n",
      "D(x): 0.0019, D(G(z)): 0.0425\n",
      "Epoch: [49/200], Batch Num: [285/1335]\n",
      "Discriminator Loss: 0.0040, Generator Loss: 8.8820\n",
      "D(x): 0.0076, D(G(z)): 0.0004\n",
      "Epoch: [49/200], Batch Num: [385/1335]\n",
      "Discriminator Loss: 0.0049, Generator Loss: 5.9502\n",
      "D(x): 0.0002, D(G(z)): 0.0095\n",
      "Epoch: [49/200], Batch Num: [485/1335]\n",
      "Discriminator Loss: 0.0022, Generator Loss: 6.6004\n",
      "D(x): 0.0001, D(G(z)): 0.0042\n",
      "Epoch: [49/200], Batch Num: [585/1335]\n",
      "Discriminator Loss: 0.0090, Generator Loss: 5.7964\n",
      "D(x): 0.0086, D(G(z)): 0.0094\n",
      "Epoch: [49/200], Batch Num: [685/1335]\n",
      "Discriminator Loss: 0.0099, Generator Loss: 6.1199\n",
      "D(x): 0.0003, D(G(z)): 0.0196\n",
      "Epoch: [49/200], Batch Num: [785/1335]\n",
      "Discriminator Loss: 0.0020, Generator Loss: 8.9775\n",
      "D(x): 0.0035, D(G(z)): 0.0004\n",
      "Epoch: [49/200], Batch Num: [885/1335]\n",
      "Discriminator Loss: 0.0013, Generator Loss: 7.6716\n",
      "D(x): 0.0010, D(G(z)): 0.0016\n",
      "Epoch: [49/200], Batch Num: [985/1335]\n",
      "Discriminator Loss: 0.0043, Generator Loss: 6.1616\n",
      "D(x): 0.0019, D(G(z)): 0.0068\n",
      "Epoch: [49/200], Batch Num: [1085/1335]\n",
      "Discriminator Loss: 0.0042, Generator Loss: 6.6671\n",
      "D(x): 0.0047, D(G(z)): 0.0038\n",
      "Epoch: [49/200], Batch Num: [1185/1335]\n",
      "Discriminator Loss: 0.0085, Generator Loss: 5.9269\n",
      "D(x): 0.0006, D(G(z)): 0.0164\n",
      "Epoch: [49/200], Batch Num: [1285/1335]\n",
      "Discriminator Loss: 0.0005, Generator Loss: 8.4029\n",
      "D(x): 0.0001, D(G(z)): 0.0009\n",
      "Epoch: [50/200], Batch Num: [50/1335]\n",
      "Discriminator Loss: 0.0022, Generator Loss: 7.1667\n",
      "D(x): 0.0005, D(G(z)): 0.0038\n",
      "Epoch: [50/200], Batch Num: [150/1335]\n",
      "Discriminator Loss: 0.0011, Generator Loss: 8.0368\n",
      "D(x): 0.0001, D(G(z)): 0.0021\n",
      "Epoch: [50/200], Batch Num: [250/1335]\n",
      "Discriminator Loss: 0.0020, Generator Loss: 8.5600\n",
      "D(x): 0.0030, D(G(z)): 0.0011\n",
      "Epoch: [50/200], Batch Num: [350/1335]\n",
      "Discriminator Loss: 0.0042, Generator Loss: 9.0391\n",
      "D(x): 0.0076, D(G(z)): 0.0008\n",
      "Epoch: [50/200], Batch Num: [450/1335]\n",
      "Discriminator Loss: 0.0020, Generator Loss: 7.4234\n",
      "D(x): 0.0018, D(G(z)): 0.0022\n",
      "Epoch: [50/200], Batch Num: [550/1335]\n",
      "Discriminator Loss: 0.0026, Generator Loss: 6.4353\n",
      "D(x): 0.0000, D(G(z)): 0.0052\n",
      "Epoch: [50/200], Batch Num: [650/1335]\n",
      "Discriminator Loss: 0.0017, Generator Loss: 8.3069\n",
      "D(x): 0.0020, D(G(z)): 0.0013\n",
      "Epoch: [50/200], Batch Num: [750/1335]\n",
      "Discriminator Loss: 0.0007, Generator Loss: 8.5485\n",
      "D(x): 0.0008, D(G(z)): 0.0007\n",
      "Epoch: [50/200], Batch Num: [850/1335]\n",
      "Discriminator Loss: 0.0022, Generator Loss: 6.8716\n",
      "D(x): 0.0001, D(G(z)): 0.0044\n",
      "Epoch: [50/200], Batch Num: [950/1335]\n",
      "Discriminator Loss: 0.0055, Generator Loss: 6.0384\n",
      "D(x): 0.0013, D(G(z)): 0.0097\n",
      "Epoch: [50/200], Batch Num: [1050/1335]\n",
      "Discriminator Loss: 0.0012, Generator Loss: 8.7697\n",
      "D(x): 0.0017, D(G(z)): 0.0008\n",
      "Epoch: [50/200], Batch Num: [1150/1335]\n",
      "Discriminator Loss: 0.0041, Generator Loss: 6.8780\n",
      "D(x): 0.0036, D(G(z)): 0.0046\n",
      "Epoch: [50/200], Batch Num: [1250/1335]\n",
      "Discriminator Loss: 0.1388, Generator Loss: 3.3452\n",
      "D(x): 0.0039, D(G(z)): 0.2737\n",
      "Epoch: [51/200], Batch Num: [15/1335]\n",
      "Discriminator Loss: 0.0082, Generator Loss: 6.3741\n",
      "D(x): 0.0068, D(G(z)): 0.0096\n",
      "Epoch: [51/200], Batch Num: [115/1335]\n",
      "Discriminator Loss: 0.0200, Generator Loss: 4.8331\n",
      "D(x): 0.0127, D(G(z)): 0.0273\n",
      "Epoch: [51/200], Batch Num: [215/1335]\n",
      "Discriminator Loss: 0.0244, Generator Loss: 7.4562\n",
      "D(x): 0.0438, D(G(z)): 0.0049\n",
      "Epoch: [51/200], Batch Num: [315/1335]\n",
      "Discriminator Loss: 0.0048, Generator Loss: 6.5433\n",
      "D(x): 0.0008, D(G(z)): 0.0087\n",
      "Epoch: [51/200], Batch Num: [415/1335]\n",
      "Discriminator Loss: 0.0097, Generator Loss: 6.4543\n",
      "D(x): 0.0128, D(G(z)): 0.0065\n",
      "Epoch: [51/200], Batch Num: [515/1335]\n",
      "Discriminator Loss: 0.0013, Generator Loss: 8.1031\n",
      "D(x): 0.0014, D(G(z)): 0.0012\n",
      "Epoch: [51/200], Batch Num: [615/1335]\n",
      "Discriminator Loss: 0.0146, Generator Loss: 7.3814\n",
      "D(x): 0.0256, D(G(z)): 0.0036\n",
      "Epoch: [51/200], Batch Num: [715/1335]\n",
      "Discriminator Loss: 0.0103, Generator Loss: 7.6960\n",
      "D(x): 0.0185, D(G(z)): 0.0021\n",
      "Epoch: [51/200], Batch Num: [815/1335]\n",
      "Discriminator Loss: 0.0025, Generator Loss: 7.3489\n",
      "D(x): 0.0030, D(G(z)): 0.0021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [51/200], Batch Num: [915/1335]\n",
      "Discriminator Loss: 0.0184, Generator Loss: 4.8706\n",
      "D(x): 0.0014, D(G(z)): 0.0353\n",
      "Epoch: [51/200], Batch Num: [1015/1335]\n",
      "Discriminator Loss: 0.0106, Generator Loss: 5.2207\n",
      "D(x): 0.0001, D(G(z)): 0.0212\n",
      "Epoch: [51/200], Batch Num: [1115/1335]\n",
      "Discriminator Loss: 0.0013, Generator Loss: 7.3322\n",
      "D(x): 0.0003, D(G(z)): 0.0023\n",
      "Epoch: [51/200], Batch Num: [1215/1335]\n",
      "Discriminator Loss: 0.0010, Generator Loss: 10.0173\n",
      "D(x): 0.0018, D(G(z)): 0.0002\n",
      "Epoch: [51/200], Batch Num: [1315/1335]\n",
      "Discriminator Loss: 0.0006, Generator Loss: 8.4791\n",
      "D(x): 0.0005, D(G(z)): 0.0007\n",
      "Epoch: [52/200], Batch Num: [80/1335]\n",
      "Discriminator Loss: 0.0035, Generator Loss: 6.3545\n",
      "D(x): 0.0007, D(G(z)): 0.0063\n",
      "Epoch: [52/200], Batch Num: [180/1335]\n",
      "Discriminator Loss: 0.0048, Generator Loss: 6.3411\n",
      "D(x): 0.0012, D(G(z)): 0.0084\n",
      "Epoch: [52/200], Batch Num: [280/1335]\n",
      "Discriminator Loss: 0.0004, Generator Loss: 10.4682\n",
      "D(x): 0.0006, D(G(z)): 0.0002\n",
      "Epoch: [52/200], Batch Num: [380/1335]\n",
      "Discriminator Loss: 0.0286, Generator Loss: 5.0549\n",
      "D(x): 0.0016, D(G(z)): 0.0557\n",
      "Epoch: [52/200], Batch Num: [480/1335]\n",
      "Discriminator Loss: 0.0035, Generator Loss: 6.9899\n",
      "D(x): 0.0004, D(G(z)): 0.0066\n",
      "Epoch: [52/200], Batch Num: [580/1335]\n",
      "Discriminator Loss: 0.0145, Generator Loss: 6.4183\n",
      "D(x): 0.0015, D(G(z)): 0.0275\n",
      "Epoch: [52/200], Batch Num: [680/1335]\n",
      "Discriminator Loss: 0.0077, Generator Loss: 9.9343\n",
      "D(x): 0.0151, D(G(z)): 0.0002\n",
      "Epoch: [52/200], Batch Num: [780/1335]\n",
      "Discriminator Loss: 0.0018, Generator Loss: 10.7450\n",
      "D(x): 0.0036, D(G(z)): 0.0001\n",
      "Epoch: [52/200], Batch Num: [880/1335]\n",
      "Discriminator Loss: 0.0045, Generator Loss: 7.5292\n",
      "D(x): 0.0064, D(G(z)): 0.0027\n",
      "Epoch: [52/200], Batch Num: [980/1335]\n",
      "Discriminator Loss: 0.0035, Generator Loss: 8.6412\n",
      "D(x): 0.0063, D(G(z)): 0.0007\n",
      "Epoch: [52/200], Batch Num: [1080/1335]\n",
      "Discriminator Loss: 0.0024, Generator Loss: 10.1843\n",
      "D(x): 0.0047, D(G(z)): 0.0002\n",
      "Epoch: [52/200], Batch Num: [1180/1335]\n",
      "Discriminator Loss: 0.0003, Generator Loss: 9.0661\n",
      "D(x): 0.0001, D(G(z)): 0.0005\n",
      "Epoch: [52/200], Batch Num: [1280/1335]\n",
      "Discriminator Loss: 0.0026, Generator Loss: 7.7120\n",
      "D(x): 0.0034, D(G(z)): 0.0017\n",
      "Epoch: [53/200], Batch Num: [45/1335]\n",
      "Discriminator Loss: 0.0017, Generator Loss: 9.5233\n",
      "D(x): 0.0031, D(G(z)): 0.0003\n",
      "Epoch: [53/200], Batch Num: [145/1335]\n",
      "Discriminator Loss: 0.0014, Generator Loss: 8.2373\n",
      "D(x): 0.0015, D(G(z)): 0.0014\n",
      "Epoch: [53/200], Batch Num: [245/1335]\n",
      "Discriminator Loss: 0.0009, Generator Loss: 7.8911\n",
      "D(x): 0.0004, D(G(z)): 0.0013\n",
      "Epoch: [53/200], Batch Num: [345/1335]\n",
      "Discriminator Loss: 0.0053, Generator Loss: 7.2778\n",
      "D(x): 0.0049, D(G(z)): 0.0056\n",
      "Epoch: [53/200], Batch Num: [445/1335]\n",
      "Discriminator Loss: 0.0115, Generator Loss: 6.1854\n",
      "D(x): 0.0004, D(G(z)): 0.0227\n",
      "Epoch: [53/200], Batch Num: [545/1335]\n",
      "Discriminator Loss: 0.1073, Generator Loss: 5.0250\n",
      "D(x): 0.0000, D(G(z)): 0.2145\n",
      "Epoch: [53/200], Batch Num: [645/1335]\n",
      "Discriminator Loss: 0.2006, Generator Loss: 8.3638\n",
      "D(x): 0.3983, D(G(z)): 0.0029\n",
      "Epoch: [53/200], Batch Num: [745/1335]\n",
      "Discriminator Loss: 0.0077, Generator Loss: 9.3307\n",
      "D(x): 0.0146, D(G(z)): 0.0008\n",
      "Epoch: [53/200], Batch Num: [845/1335]\n",
      "Discriminator Loss: 0.0116, Generator Loss: 5.6721\n",
      "D(x): 0.0082, D(G(z)): 0.0149\n",
      "Epoch: [53/200], Batch Num: [945/1335]\n",
      "Discriminator Loss: 0.0008, Generator Loss: 8.7371\n",
      "D(x): 0.0002, D(G(z)): 0.0013\n",
      "Epoch: [53/200], Batch Num: [1045/1335]\n",
      "Discriminator Loss: 0.0033, Generator Loss: 8.0989\n",
      "D(x): 0.0050, D(G(z)): 0.0016\n",
      "Epoch: [53/200], Batch Num: [1145/1335]\n",
      "Discriminator Loss: 0.0021, Generator Loss: 7.2490\n",
      "D(x): 0.0023, D(G(z)): 0.0020\n",
      "Epoch: [53/200], Batch Num: [1245/1335]\n",
      "Discriminator Loss: 0.0048, Generator Loss: 7.6362\n",
      "D(x): 0.0023, D(G(z)): 0.0073\n",
      "Epoch: [54/200], Batch Num: [10/1335]\n",
      "Discriminator Loss: 0.0015, Generator Loss: 8.5128\n",
      "D(x): 0.0022, D(G(z)): 0.0008\n",
      "Epoch: [54/200], Batch Num: [110/1335]\n",
      "Discriminator Loss: 0.0101, Generator Loss: 5.2310\n",
      "D(x): 0.0014, D(G(z)): 0.0188\n",
      "Epoch: [54/200], Batch Num: [210/1335]\n",
      "Discriminator Loss: 0.0161, Generator Loss: 8.2546\n",
      "D(x): 0.0312, D(G(z)): 0.0011\n",
      "Epoch: [54/200], Batch Num: [310/1335]\n",
      "Discriminator Loss: 0.0021, Generator Loss: 8.1214\n",
      "D(x): 0.0030, D(G(z)): 0.0011\n",
      "Epoch: [54/200], Batch Num: [410/1335]\n",
      "Discriminator Loss: 0.0009, Generator Loss: 7.6851\n",
      "D(x): 0.0000, D(G(z)): 0.0018\n",
      "Epoch: [54/200], Batch Num: [510/1335]\n",
      "Discriminator Loss: 0.0024, Generator Loss: 8.6922\n",
      "D(x): 0.0042, D(G(z)): 0.0007\n",
      "Epoch: [54/200], Batch Num: [610/1335]\n",
      "Discriminator Loss: 0.0037, Generator Loss: 7.2875\n",
      "D(x): 0.0048, D(G(z)): 0.0026\n",
      "Epoch: [54/200], Batch Num: [710/1335]\n",
      "Discriminator Loss: 0.0012, Generator Loss: 8.5022\n",
      "D(x): 0.0013, D(G(z)): 0.0011\n",
      "Epoch: [54/200], Batch Num: [810/1335]\n",
      "Discriminator Loss: 0.0009, Generator Loss: 9.7817\n",
      "D(x): 0.0015, D(G(z)): 0.0002\n",
      "Epoch: [54/200], Batch Num: [910/1335]\n",
      "Discriminator Loss: 0.0031, Generator Loss: 6.5882\n",
      "D(x): 0.0007, D(G(z)): 0.0056\n",
      "Epoch: [54/200], Batch Num: [1010/1335]\n",
      "Discriminator Loss: 0.0007, Generator Loss: 8.5793\n",
      "D(x): 0.0008, D(G(z)): 0.0006\n",
      "Epoch: [54/200], Batch Num: [1110/1335]\n",
      "Discriminator Loss: 0.0018, Generator Loss: 7.3678\n",
      "D(x): 0.0017, D(G(z)): 0.0019\n",
      "Epoch: [54/200], Batch Num: [1210/1335]\n",
      "Discriminator Loss: 0.0032, Generator Loss: 10.5647\n",
      "D(x): 0.0064, D(G(z)): 0.0001\n",
      "Epoch: [54/200], Batch Num: [1310/1335]\n",
      "Discriminator Loss: 0.0010, Generator Loss: 8.4512\n",
      "D(x): 0.0009, D(G(z)): 0.0011\n",
      "Epoch: [55/200], Batch Num: [75/1335]\n",
      "Discriminator Loss: 0.0002, Generator Loss: 9.6599\n",
      "D(x): 0.0002, D(G(z)): 0.0002\n",
      "Epoch: [55/200], Batch Num: [175/1335]\n",
      "Discriminator Loss: 0.0006, Generator Loss: 8.1524\n",
      "D(x): 0.0004, D(G(z)): 0.0007\n",
      "Epoch: [55/200], Batch Num: [275/1335]\n",
      "Discriminator Loss: 0.0014, Generator Loss: 7.4307\n",
      "D(x): 0.0005, D(G(z)): 0.0022\n"
     ]
    }
   ],
   "source": [
    "# for testing DCGAN\n",
    "num_test_samples = 12\n",
    "test_noise = to_cuda(Variable(torch.Tensor(np.random.normal(0, 1, (bSize, latent_dim, 1, 1)))))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (imgs, _) in enumerate(data_loader):\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        \n",
    "        valid = to_cuda(Variable(torch.Tensor(imgs.shape[0], 1, 1, 1).fill_(1.0), requires_grad=False))\n",
    "        fake = to_cuda(Variable(torch.Tensor(imgs.shape[0], 1, 1, 1).fill_(0.0), requires_grad=False))\n",
    "\n",
    "        # Configure input\n",
    "\n",
    "        real_imgs = to_cuda(Variable(imgs.type(torch.Tensor)))\n",
    "   \n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        g_optimizer.zero_grad()\n",
    "\n",
    "        # Sample noise as generator input\n",
    "        z = to_cuda(Variable(torch.Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim, 1, 1)))))\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z)\n",
    "        \n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        g_loss = loss(discriminator(gen_imgs), valid)\n",
    "\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        d_optimizer.zero_grad()\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        real_loss = loss(discriminator(real_imgs), valid)\n",
    "        fake_loss = loss(discriminator(gen_imgs.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "\n",
    "        # Display Progress every few batches\n",
    "        n_batch = epoch * len(data_loader) + i\n",
    "        if (n_batch) % 100 == 0: \n",
    "            test_images = vectors_to_images(generator(test_noise), 1, dwidth, dheight)\n",
    "\n",
    "#             logger.log_images(\n",
    "#                 test_images, num_test_samples, \n",
    "#                 epoch, n_batch, num_batches\n",
    "#             );\n",
    "            # Display status Logs\n",
    "            logger.display_status(\n",
    "                epoch, num_epochs, i, len(data_loader),\n",
    "                d_loss, g_loss, real_loss, fake_loss\n",
    "            )\n",
    "            save_image(gen_imgs.data[:25], \"dc-images/%d.png\" % n_batch, nrow=5, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
